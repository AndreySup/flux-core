March 4, 2013 - NGRM high level review

Attendees: Kim Cupps, Pam Hamilton, Mark Gary, Jeff Long,
	Chris Morrone, Ned Bass, Martin Schulz, Scott Futral,
	Dong Ahn, Jim Garlick, Mark Grondona, Don Lipari

Two drafts of "Vision" document were reviewed:
- Sec 1-4 only as of Feb 27, 2013, tagged 'vision-review-1'
- Full document as of Feb 19, 2013, tagged 'vision-review-2'

Slides were presented:
- intro (Jim)
- comms (Jim)
- res mgmt (Mark)
- scheduler (Don)
(skipped monitoring slides)
- WRAP (Dong)

Comments collected from reviewers:

* Kim: WRAP section uses different terminology than other sections
  and is hard to get through.

  - margin comment added to sec 8

* Martin: The term "comms" will be confusing to the MPI community.

  - retained terminology but added footnote to disambiguate.
    This layer won't be exposed to MPI runtimes or users (see WRAP).

* Kim: Fault tolerance should be more fully designed in the comms layer
  before moving forward.

  - margin comment added to sec 5.4

* Kim/others: The system administration paragraph at the end of Center
  as a Cluster is not well supported by the rest of the document.

  - paragraph replaced with:

  This paradigm also has many possible advantages for system administration.
  The resource inventory for the center is managed from a central point
  and contains details that can drive center-wide configuration management.
  A "cluster" is diminished as a primary, user-facing data center entity
  and instead can be viewed as an arbitrary collection of resources, part of
  a larger system, that happens to be attached to a single interconect domain
  or that have other similar characteristics.
  A cluster downtime, formerly viewed as a period of unavailability,
  can be viewed in the new paradigm as a period of degraded performance.
  Through lightweight virtualization, users obtain a degree of independence
  from system software updates, which in some cases can quietly roll out
  across the center between jobs with minimal impact.
  As the new paradigm embraces heterogeneity within the larger system,
  new resources can be purchased and added on, as dictated by demand,
   without the need to build a standalone cluster entity, separately named
  and managed, for every new type of hardware introduced.
  In short, system administration activities can take place in a more
  centralized, less visible manner such that they are no longer perceived
  by users as at odds with their productivity.

* Chris: Do you really want to say that dedicated interactive nodes
  will be unnecessary (under center as a cluster) since users will have
  to login to NGRM to request (custom) interactive access somehow.

  - dropped sentence.

* Chris: Due to the resource/job/user db persistence model (reaped at end
  of job) does a job that crashes loose too much data?  Should there be
  updates pushed toward the persistent store while a job is running?

  - See "6.5.5 Job Fault Tolerance".  Design is TBD but some possibilities
    for resilience of the databases are briefly discussed.

* Chris: Is the "lightweight virtualization" / "virtual node" too
  Linux-specific, i.e. would requiring it as part of the design
  inhibit portability to non-Linux systems e.g. exascale?

  - margin comment added to sec 3.2, also:
  - Portability to non-Linux systems was not a goal of the project,
    however OS-level virtualization is available in FreeBSD jails, Solaris
    zones, and AIX WPARS.  It will likely be implemented through NGRM plugins
    allowing for some portability or replacement with other container methods
    such as full virtualization, chroot jails, etc.

* Chris: Is the root of the tree in the CMB design a single point of
  failure?

  - margin comment added to sec 5.3, also:
  - This is one of the issues that will have to be addressed in
    our fault tolerance design for the CMB, a future work item.

* Chris: Why should comms session forget parent's key?   Is a new
  session formed for each level of recursion or is there a single thing?
  Document seems to imply the latter.

  - margin comment added to sec 5.2, also:
  - The high level concept is that the portion of the CMB running
    on each node is associated with one session at a time.  It forgets the
    parent context when it joins a child session.  However the parent retains
    the context of its children so it can interact directly with a child node
    if necessary, for example to reclaim resources from a hung session.

* Chris: Expand PGM acronym (in 0MQ discussion) and define.

  - Expanded to "Pragmatic General Multicast (PGM) [RFC citation],
    a reliable multicast transport protocol,..."

* Chris: In shared TBON (comms), be careful that user contributed traffic
  e.g. stdio doesn't starve out system control messages.  Also avoid situation
  encountered with srun where putting srun to sleep causes hierarchical
  communication to get backlogged.

  - margin comment added to sec 5.4

* Does DAT stand for "Dedicated Application Time" or "Test"?

  - updated two occurrences to use "Time" per
    https://computing.llnl.gov/tutorials/lc_resources/

* Chris: Why are "job prioritization" and "job scheduling" independent
  activities?  Shouldn't prioritization just be one aspect of scheduling?

  - margin comment added to sec 6.6.2

* Chris: Plugins described in scheduling section seem like they are
  each pigeon-holed for a particular function, compared to SPANK where
  one plugin is called in different contexts to provide a suite of
  related functions.

  - margin comment added to sec 6.6

* Chris: There should be multiple levels of preemptability.

  - margin comment added to sec 6.6.2

* Chris: Why do we need a hierarchy of LWJ's?  Are we reinventing
  the same stuff we already built for jobs?  Can't we just do LWJ stuff
  with jobs?

  - margin comment added to sec 8.1

* Jeff: (data provenance) To do this 'right', each job would have to have
  a true UUID, which is an ugly thing to work with for humans. So maybe
  there'd be a regular job id that's local to the domain (cz, rz, etc.)
  that is just an auto-incrementing id, plus the 16-byte uuid that truly
  is unique?

  - margin comment added to sec 6.3.3

* Jeff: (data provenance) There have been efforts in the past to maintain
  provenance for simulations, but the ones that didn't fail were cumbersome.
  Having some of these features built into the RM is pretty cool and could be
  one of the nice value-added features in your selling points slide.

  - margin comment added to sec 2

* Jeff: (data provenance) If users can add their own data to the provenance
  database, how is long-term storage managed, especially if users decide
  they want to store a movie or data file as part of the permanent record?

  - margin comment added to sec 6.3.3

* Jeff: (project questions) Will the different thrust areas be developed
  in parallel by different people?

  - could be done with some co-design between thrust areas, and a
    project-wide review process, however the actual plan (including
    staffing) is up in the air right now.

* Jeff: (project questions) I can no longer see/find your confluence space.
  Just wanted to make sure that was intended, or that I'm experiencing pilot
  error.

  - We moved everything to github and removed the confluence space
    since our intent is to make this a multi-lab project.  Everything that
    we had in confluence was incorporated into the vision document.

* Jeff: (notification) MyLC and the new HM will both be providing
  notifications for various events, so we need to develop a centralized
  mechanism that can support this and make sure NGRM can be plugged-in as
  an event source. (Sounds like the pub-sub model Mark described would work
  nicely.)

  - margin comment added to sec 7.4.  Also added:
  - FIXME: This section describes how NGRM can act as an SNMP event source
    (trap) and provide SNMP state transfer (getbulk) to external monitoring.
    Missing is the ability to go the other way, e.g. get events and state
    on externally monitored objects.
  - FIXME: Other external protocols could be supported by other gateway
    implementations.  The gateway is just a thin layer between resource
    db and external protocol engine.

* Jeff: (monitoring) How do we make sure that the new HM (called "CAVE")
  will tie in nicely with NGRM's monitoring needs? Maybe start with Tim
  attending discussions you guys have, and someone from your team attending
  Tim's meetings. Certainly the requirements should be shared between the two
  efforts. This leverage could be a nice win.

  - Agreed.  NGRM wants to replace all compute node resident data
    capture in order to control the noise, so we need to work with the
    CAVE project to plan for a transition from current data capture (skummee,
    cerebro) to NGRM.  If CAVE needs to drive new data capture before NGRM
    is ready, then we ought to work together on interim methods that minimize
    code waste.  Tim and I have communicated about this and he has a copy
    of our document's monitoring section so I believe we are synced.

* Jeff: (repositories) Resource Inventory - We as a center need one go-to
  place to find resource inventory information. We have multiple web pages,
  spreadsheets, and files that are being maintained separately right
  now. It'd be nice if your resource inventory DB was the central place
  for this info -- what are the resources in the center, what are their
  properties, etc. I love the idea of being able to subscribe to this
  repo to be alerted of changes -- much better than the current model
  where someone has to notice or remember a change was made.

  - margin comment added to sec 6.3.1

* Jeff: (user repository) Any advantage/possibility in leveraging LC's LDAP
  for this?

  - margin comment added to sec 6.3.2

* Jeff: (job repository) Currently SLURM bogs down if a person does an
  extensive long-running query (sreport) against the job database; how
  would your design avoid this?

  - margin comment added to sec 6.3.3
