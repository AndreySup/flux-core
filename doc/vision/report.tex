\documentclass{article}

\usepackage{verbatim}
\usepackage{calc}
\usepackage{epsfig}
\usepackage{url}
\usepackage{draftcopy}
\usepackage{longtable}
\input{pstricks}
\usepackage{multirow}

\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{(\paperwidth-\textwidth)/2 - 1in}
\setlength{\topmargin}{(\paperheight-\textheight -\headheight-\headsep-\footskip)/2 - 1in + .5in }

\newif\ifwbs
\wbsfalse

\newcommand{\ngrm}{NGRM}
\newcommand{\ngrmfull}{Next Generation Resource Manager}
\newcommand{\ngjs}{NGRM Job Scheduler}
%\newcommand{\zMQ}{${\varnothing}$MQ}
\newcommand{\zMQ}{\O{}MQ}
\newcommand{\slurm}{Slurm}
\newcommand{\moab}{Moab}

%\includeonly{monitor}

\begin{document}

\title{Vision and Plan for a \ngrmfull}
\author{\
Dong H. Ahn, ahn1@llnl.gov\\
Jim Garlick, garlick@llnl.gov\\
Mark Grondona, mgrondona@llnl.gov\\
Don Lipari, lipari@llnl.gov}

%\date{Nov 6, 2012}

\maketitle

\section{Overview}
\label{sect:overview}
Resource Management (RM) software is critical for High Performance Computing
(HPC). It is the centerpiece that allows efficient execution of HPC applications
while providing an HPC center with the main means to maximize 
the utilization of its computing resources. However, several growing trends make
even the best-in-breed RM software largely ineffective. As numbers and
types of compute cores of HPC systems continue to grow, key RM challenges
associated only with today's {\em capability-class} machines are 
becoming increasingly pervasive for {\em all} computing resources including 
commodity Linux clusters. The challenges include having to provide 
extreme scalability, low noise, fault tolerance, and heterogeneity management
while under a strict power budget.

In addition, greater difficulties in code development on larger systems have
begun to impose far more complex requirements on the RM. For example,
without adequate RM support, debugging, tuning, testing and verification
of the applications have become too difficult and time-consuming for end-users.
The next-generation code development environments require the RM to provide
effective mechanisms to support the reproducible results of program execution, to
provide accurate correlations between user-level errors and system-level
events, and to integrate and accelerate a rich set of scalable tools.

Further, a greater interplay among various classes of clusters across
the entire center makes the current practice of single-cluster scheduling
far less appealing. An application running on a compute cluster heavily
utilizes site-wide shared resources such as I/O and visualization clusters.
Thus, avoiding any significant site-wide bottleneck requires the RM
to schedule the job to all dependent resources together. In short, without
the RM that can effectively address all of these challenges, it has become apparent
that HPC centers will suffer a significant loss in both user productivity
and efficient uses of next-generation computing resources.

\slurm\cite{SlurmDesign} is arguably the
best-in-breed, open-sourced RM designed for commodity Linux clusters.
Livermore Computing (LC) at Lawrence Livermore National Laboratory (LLNL)
designed and led its implementation in 2002 and since then has facilitated
its wide adoption outside of LLNL.
\slurm's original design was for a 1-2K node Linux cluster with a single
interconnect domain and bulk-synchronous workload based on MPICH.
In the last decade, \slurm\ has been adapted
to act as a scheduling and job submission layer on top of proprietary
RM software on IBM Blue Gene and Cray systems,
to support other runtimes quite different from MPICH,
to run (natively) on Linux clusters pushing beyond the designed node count,
and to be tied into a \moab\cite{MOAB:online} grid.
These adaptations have accrued without fundamentally changing the
core \slurm\ design, and as a result \slurm\ is increasingly
unable to adapt to emerging requirements without compromising
stability and code maintainability.

Our response to this critical need is the \ngrmfull\ (\ngrm ), an RM software
{\em framework} that can solve the key emerging challenges 
in a simple, extensible, distributed and autonomous fashion.
It aims at managing the whole center as one common pool of {\em diverse} 
resources. Hence, scheduling decisions will be 
far more efficient as well as flexible to accommodate 
emerging constraints such as a strict power bound. 
Further, \ngrm\ integrates
system monitoring, system administration, lightweight
virtualization, and distributed tool communication capabilities
that are currently provided by disjoint and often overlapping software.
Integrations of these facilities within the common framework designed from
the ground up for scalability, security, and fault tolerance will result
in a more efficient and capable system.

We realize that \ngrm's problem space is huge and yet we must address a wide range 
of challenges in relatively short order with limited resources. Thus, we 
organize our project plan around four research and development thrust areas 
and seek to advance them systematically.
These areas are called:
{\bf{\em Resource Management}},
{\bf{\em Monitoring and Data Management}},
{\bf{\em Workload Runtime And Placement}}, and
{\bf{\em Communications Framework}}.
They are relatively independent but can significantly build on the
strength of one another through well-known interfaces and common design
and development principles.
Thus, reaching all major milestones of these thrust areas 
will represent the completion of the first round of \ngrm\ development. 

Overall, \ngrm\ will significantly improve operational efficiency for
scientific application development and execution, and further for computing
resources of the entire HPC center.  It will also provide
a foundation for further extension and customization, allowing agile responses
to site-specific scheduling issues. Perhaps more importantly, \ngrm\
positions us to cope with a blend of interrelated, diverse
extreme-scale computing resources, the landscape of high-end HPC centers
in just a few years down the road

The rest of the paper is organized as the following.
Section~\ref{sect:vision} presents \ngrm's vision and 
its new capabilities in more detail. Section~\ref{sect:projorg}  
describes the high-level goals of the four thrust areas and
explains relations among the areas. Next sections then 
go over and detail each of these areas including 
its work breakdown structure (WBS) and associated work items.
Finally, Section~\ref{sect:delivery} defines
our schedules for milestones and deliverables.


\section{Vision and New Capabilities}
\label{sect:vision}

The vision of \ngrm\ is to create a scalable RM software system that 
drastically improves operational efficiency and user productivity 
for workloads on {\em capacity-class} compute systems.
With a trend towards ever-growing numbers and types of compute cores, however,
this system class has been subject to the challenges that
today's {\em capability-class} machines have been facing. 
These challenges include having to provide extreme scalability, low noise, 
fault tolerance, and heterogeneity management while under a strict power budget.
Worse, the workloads themselves are also becoming increasingly diverse, 
dynamic and large. Thus, fully realizing our vision through these challenges requires
a {\em paradigm shift} in how the RM should manage, model, schedule 
and allocate its resources.

In the new paradigm, the RM must be capable of imposing highly complex resource bounds
to guarantee the highest operational efficiency at any level
across the computing facility, while at the same time enabling most efficient execution
and scheduling of the workloads within these bounds.
Thus, the RM must manage the entire facility as one
common pool of resources. The ability to see a broader spectrum of resources 
and their various constraints can then lead to most efficient scheduling strategies
and execution environments. Further, the same ability will ease 
efforts to diagnose errors for both end users and support staff
by associating jobs with other facility-wide events. 
The new paradigm also demands that the RM model
various types of resources and their relationships beyond the traditional resource model:
i.e., a simple collection of compute nodes.
The rich resource model will allow the RM to allocate computing resources
tailored to the disparate limiting factors of our applications: e.g.,
an application may be compute-bound while others are I/O-bound or power-bound.
Under the new paradigm, the resource allocations must also be elastic. 
An application may have different phases with disparate performance-limiting factors;
it must be able to grow and shrink its resource allocation dynamically. 
The global resource view, rich resource model and elasticity represent
the fundamental characteristics of the RM under the new paradigm.

Further, the new paradigm must provide a central framework to integrate
other relevant software. The software components should include   
system monitoring and administration, lightweight virtualization, 
and scalable tool communication. We envision that the integration will 
facilitate a higher level of leverage among these essential computing elements, 
and this will lead to significantly higher productivity 
for both end users and system adminstrators.  
In addition, as these capabilities are currently provided through disjoint
and often overlapping software, the integration will substantially reduce
the costs needed for developing and maintaining individual software.  
In the following, we further detail the key ideas and new capabilities 
under the new paradigm.


\paragraph{Center as a Cluster:}
Unlike the traditional approach of running an RM instance on each separately
managed cluster, 
%and tying clusters together with grid software almost was
%an afterthought. 
the new paradigm must manage the
entire computing facility as one pool of resources. There must be a single site-wide 
system image from the perspective of users as well as system administrators.
With this approach, file system servers such as Lustre clusters, 
and visualization and serial batch systems can be aggregated
with compute clusters into one management domain. Hence,
our RM can make better global scheduling decisions. 
Further, dedicated interactive nodes will be unnecessary in the new system, as it
will be possible to allocate interactive environments on demand from the
global pool of resources.

With integrated, center-wide monitoring, it becomes easier
for the new paradigm to associate site-wide Reliability, Availability and Serviceability~(RAS) 
events such as a global file system
failure with an affected job and to make that information part of
the job's data {\em provenance} record. When the RM obtains a global view of
resources including shared persistent storage, it becomes possible to
consider the scheduling of I/O along with computation. In combination with
I/O forwarding software, our RM could easily set up unique I/O forwarding
topologies for each job.

This paradigm also has many advantages for system administration.
The system management overhead that currently scales with the number of clusters
could be streamlined by aggregating per-cluster services into center-wide
services. For example, at LLNL, each cluster has a pair of management
nodes that are bootstrapped from scratch with an OS installation from DVD,
and when system software is updated on that cluster, a downtime is
scheduled during which the workload sits idle and the responsible system
administrator performs a sequence of steps. Under the new paradigm, the
number of systems that have to be brought up from scratch will be
greatly reduced, and administrator-orchestrated downtimes for software
updates could be eliminated. 


\paragraph{Diverse Compute Resources:}
The traditional paradigm has solely focused on node- and/or CPU-centric
scheduling. With the advent of hybrid compute systems utilizing specialized,
heterogeneous resources and other bounding resources like power, 
this simple model has become less effective. Rather than
perpetuating a node-centric resource model with support for other
resources as an add-on, the new paradigm must fully support diverse resources. 
Thus, the idea of a resource must be kept as generic as
possible. This will not only faciliate simpler handling of generic resources,
but also enable future expansion to resource types that have yet to
be conceived. New resource types can be provided through configuration changes
and/or simple extensions that inherit their attributes from base types 
to maximize reuse and foment collaboration.  Various resource topologies
can be encoded via configuration, and resources can also be allowed to have tags or labels
to which resource requests may refer. 
The new paradigm must provide a generic resource query language to allow flexible
specification o?f resource requirements.


\paragraph{Data Provenance and Reproducibility:}
As simulation plays an increasingly central role in scientific
investigation, reproducibility of results is more important than ever
before. A result should be accompanied by a data provenance record that
can be used by others to recreate the inputs and conditions that led to
that result. It should also record unusual system activities 
such as RAS events that might help in a post-mortem analysis 
when expected results are not obtained.
The new paradigm must produce such a record for every job.
Long running parameter studies or uncertainty quantification runs
require stability for long periods of time. 
% DONG: This can move to our model or design
%Private filesystem
%namespaces, as described above, enable applications to lock down their
%environment including dependent shared libraries so that the effects from
%system updates on the application are minimized.
%


%DONG: This can go to the design space discussion and desing/model
%\subsection{Zero Downtime}
%The impact of downtime becomes greater with a higher degree of integration
%of systems and services. Thus, \ngrm\ must be tolerant
%of hardware and software faults and failures.
%It will be designed to have no single point of failure and support
%version interoperability that allows {\em live} software upgrades.
%The technique will faciliate a rolling update across the center
%without impacting overall center availability and/or running workloads.
%
%\ngrm\ will support the notion of lightweight containers 
%and private filesystem namespaces for jobs, achieving 
%a higher degree of isolation between system software and user-visible 
%software. For example, the RM and the Operating System (OS)
%kernel might see one version of the root filesystem, while an application
%might see another. The decoupling and isolation will be the key to the 
%much desired ability to upgrade the software levels in the machine's
%root filesystem without affecting any of the
%libraries that an application might be using, and vice-versa. 
%Similarly, one can upgrade the system OS image at idle points between jobs,
%with user-level software remaining unchanged, or vice versa. The separation of
%concerns gives more flexibility to the organization in determining software
%update policy and in fact could allow users or code development teams to
%control the software levels affecting their application, independent of
%other applications.
%

\paragraph{Low Noise:}
As the number of processes in a parallel application increases, 
their executions are affected to varying degrees by OS
scheduling jitter, depending on their communication patterns. Minimizing
the user-space system software contribution to OS jitter must one 
of the primary goals of the new paradigm.  Thus, the new RM must supplant
the independent monitoring, remote shell, and cron
services that contribute to noise today. The integrated services will
allow users to dial up or down the verbosity and frequency of monitoring,
depending on their debug/monitoring needs versus their application's noise
sensitivity. Cron (periodic housekeeping) jobs and rsh (remote command
executions) can be performed through the RM to minimize their impact,
such as running them between jobs or synchronized across jobs. 
The new RM must also be flexible enough to allow implementation of other
strategies for reducing the impact of noise, such as scheduling all
non-application activity to a configurable {\em noise core}.


\paragraph{Fault Torelance:}
As the new paradigm manages the entire compute facility,
the RM's torelance to haredware and software faults and failures
must become one of the primary attributes.
Thus, the RM must have no single point of failure and support
version interoperability that allows {\em live} software upgrades.
The paradigm must faciliate a rolling update across the center
without negatively impacting overall availability of 
the facility and/or running workloads.


\paragraph{Security:}
The new paradigm must continue to support and strengthen privacy 
and integrity on
the network to limit vulnerability to attacks involving physical access
to a system or its networks.

%DONG: This can go to the design/concept
%Lightweight containers will tightly control the access that
%applications have to system resources. For example, it is possible to
%run the job in its own network namespace such that direct access to the
%system management network is unavailable, or limited by node firewall
%rules. \ngrm\ could also use the private network namespace to isolate
%jobs from each other on virtual private networks.  
%Containers with private filesystem namespaces can limit visibility of
%filesystems to jobs based on the site policy. 
%In combination with I/O
%forwarding software, \ngrm\ could squash all access to filesystems to
%the user id of the job so privilege escalation within the job such as
%escaping a container or obtaining root within the container would not
%change the user's identity when accessing the filesystem.
%


\paragraph{Research and Tool Friendly:}
The new paradigm must facilitate development and use of 
scalable run-time code development tools. These tools can significantly 
improve end-user productivity, who must develop, debug, optimize, 
test and verify their code on the next-generation systems. 
Similarly, it must
also facilitate research with the same goal in mind.
Specifically, the new paradigm must support highly scalable infrastructure
and rich run-time interfaces
on which tools can build. Futher, it must have 
the ability to capture and
publish sanitized system data at all levels to facilitate the use of
this data in current and future research. 


\paragraph{Extensibility:}
By all means, such a paradigm shift is an ambitious goal, and thus
the new RM must be extensible and customizable to help achieve this goal.
It must provide features that lower the barrier of entry into the
community of developers supporting and extending the RM.
Plugins, when designed properly, can signicantly help realize this vision,
without having to sacrifice the stability of the core RM software. 
With plugins, various RM subsystems can be replaced and extended.  
That can be the mechanisms for
sites to customize their RM for their particular needs. 
In addition, the new RM must be designed to be self-hosting: it must  
be capable of launching a standalone copy of itself as a job. This makes
it possible for developers or a QA team to test new RM versions. 

%Components of \ngrm\ will be designed with the ability to capture and
%publish sanitized system data at all levels to facilitate the use of
%this data in current and future research.  The plugin and self-hosting
%features of the system will encourage experimentation and
%facilitate research activities by allowing new and experimental
%versions of the software to be run within a production instance.
%One of our design goals is also to facilitate the
%development and use of scalable runtime code development tools 
%on jobs on next-generation computing systems. This will not only serve 
%to lower the barrier to create and deploy useful tools, but will also reduce the
%profusion of one-off software systems in use on production systems.
%

\section{Design Space and New Models}

Before going into the design and implementation details of \ngrm, we discuss 
some of the key design challenges that the new RM paradigm presents. 
They represent the main factors that \ngrm's new concepts and software design
must effectively address in realizing the vision and new capabilities described in 
Section~\ref{sect:vision}: 
\begin{itemize}
\item{\sl Multi-dimensional scale challenge:} The new paradigm demands that 
      the RM must manage the entire computing facility as one common pool of 
      resources. Compared to the traditional paradigm, this presents 
      fundamentally more difficult scale challenges to the RM design,
      not only in the concurrency of a sinlge workload but along
      several other dimensions. As concurrency increases, every RM run-time
      service must scale and noise must be put at bay.  
      The number of jobs and resources 
      that the RM must manage will drastically increase; the amount 
      of runtime information that the RM must monitor, trace and store 
      will grow in the scaling limit of the facility.
      Thus, this challenge precludes any centralized design in an attempt to 
      gain a wider view over the resources at the facility. 

\item{\sl Diverse workload challenge:} The new paradigm must recognize that
      different applications have different performance-limiting factors,
      and this imposes more complex requirements to how the RM should
      model the compute resources. The traditional approach of modeling 
      resources as a collection of compute nodes will only work well when the
      application is compute-bound. Modern workloads have grown in their
      complexity, and even today, only a small fraction of modern applications
      are compute-bound.
       
\item{\sl Dynamic workload challenge:} Not only must the paradigm support 
      disparate performance limiters across different applications, but
      also must it be well-suited for varying performance limiters within 
      a single application. Our applications have grown to be more
      dynamic with different resource requirements at different
      phases.

\item{\sl Power challenge:} As one specific example of emerging resource types,
      power is becoming increaingly important. When the computing
      facility becomes power-bound instead of compute-node-bound, the new
      paradigm must be able to help it to schedule based upon the 
      maximum power limit at any level at the facility. Thus, 
      the resource representation of the new RM must be generalized
      enough to model consumable resources like power.  
      
\item{\sl Scheduling challenge:} As more diverse attributes of resources
      are factored into scheduling, more stalls can occur in the schedule.
      For instance, $N$ compute nodes may sit idling simply because they do not meet
      the network proximity requirement for a workload that requires 
      $N$ nodes connected at a same lower-level switch. Thus, our design 
      must provide alternative ways to fill the stalls to meet this 
      challenge.

\item{\sl Backward compatibility challenge:} The new paradigm must also be
      able to model the traditional paradigm, as its small subset. This
      then provides our design with a straightfoward path to 
      backward compatibility with legacy scripts from a traditional 
      paradigm such as \slurm.

\item{\sl Integration risk:} In the new paradigm, the RM must 
     integrate other software essential to the next-generation computation. 
     But with higher integration comes the risk of hardwiring assumptions
     that later prove to be confining. That can force changes down the road
     that are inconsistent with the initial design. This motivates
     an extendible framework design. 
     
\item{\sl Higher downtime costs:} The impact of downtime under the 
     new paradigm becomes much greater: if not designed adequately, 
     a downtime can negatively affect the availability of
     a large portion of the facility and/or running workloads across it.
     Thus, the new paradigm must be tolerant of hardware and software faults 
     and failures with no single point of failure and must support version 
     interoperability that allows {\em live} software upgrades.

\item{\sl Separation-of-concerns challenge:} Many attributes of the new
     paradigm motivate a much higher degree of separation between
     the software level visible to applications and system-level 
     software. For example, the paradigm must be able to reconstruct the user-visible
     software level to provide better reproducibility of simulations while not
     locking the system software level. 
   
\item{\sl Security challenge:} As the new paradigm increasingly motivates
     a more distributed and hierachical software design, 
     the importance of security across and within the components becomes greater.

\item{\sl Productivity challenges:} The new paradigm must improve end-user 
     productivity in part through first-class support for development
     and support for scalable code development run-time tools and research.

\end{itemize}

DRAFT HIGH LEVEL MODELS HERE?? or "HIGH LEVEL DESIGN" BELOW CAN COVER ALL THAT. 

\begin{itemize}
\item{Hierarchical job model:} Describe brefly what an NGRM job is; what the hierarchy among them means. This is the umberalla model that addresses the bulk of {\sl Multi-dimensional scale challenge}.  

\item{Generalized resource model:} Describe resource description and specification language as well. This is the model that addresses {\sl Diverse workload challenge}, {\sl Dynamic workload challenge}, {\sl Power challenge}, {\sl Scheduling challenge}   

\item{Elasticity model:} Describe our grow/shrink model. This is the model that addresses {\sl Dynamic workload challenge}, {\sl Power challenge}, {\sl Scheduling challenge}

\item{Common scalable infrastructure model:} Including per-job Comms session. This is the model that addresses scale challenge in concurrency along with other new capabilities noise control. {\sl Integration challenge} {\sl Productivity tools challenge}

\item{Self-hosting model:} Describe a job can custimize itself with the self-hosting capabilty and its own plug-ins. Explain how that can address {\sl Integration risk} and {\sl Productivity challenge} . 

\item{Rich, scalable run-time model:} Perhaps describe some essence of WRAP. {\sl scale challenge} and {\sl Productivity challenge}.  Also explain that through LWJ, it also address other key challenges like dynamic, power challenges...  

\end{itemize}

\section{Next Generation Resource Manager}
\subsection{Terminology}

\paragraph{comms session}
An established communication association among a set of nodes that
enables secure, scalable, elastic, and fault-resilient communication
services for an \ngrm\ {\bf instance}.
A comms session is identified by its private DNS name, e.g. s1.\ngrm.
or s1.s1.\ngrm.

\paragraph{confinement}
The enforcement of resource allocations such that an {\bf instance} cannot
consume more than was allocated.  Confinement of an instance to assigned
resources is the responsiblility of the parent and, in the case of inherited
confinment, grandparent instance(s).

\paragraph{control node}
A distinguished node within a {\bf comms session} which holds the master
{\em comms state}, is the root of the session's aggregation/reduction tree,
and which performs gateway functions with the parent session.

\paragraph{instance}
An independent set of resource manager services configured to manage
a set of resources.
Instances are created dynamically and recursively.
We refer to the bootstrap {\em root instance} which contains all resources,
and speak of {\em parent}, {\em child}, and {\em sibling} relations between
instances.
An instance is identified by its {\bf comms session} name,
e.g. s1.\ngrm.  or s1.s1.\ngrm.

\paragraph{job}
A time-bounded allocation of resources.  A job is submitted to a
particular {\bf instance}.  When it executes, a child instance is
created to contain the job.  As a special case the {\em root job} 
runs perpetually without a time bound.
An job is identified by its {\bf instance} name,
e.g. s1.\ngrm.  or s1.s1.\ngrm.

\paragraph{lightweight job} (LWJ) 
A {\bf job} submitted and recorded in the current {\bf instance}
which does not result in a new instance.

\subsubsection{Comparisons with Traditional Terminology}

While the above terminology serves to define the constructs of the
NGRM system, table ~\ref{tab:tradterms} serves as a bridge to concepts
used in traditional batch schedulers and resource managers.

\begin{table}
\caption{Terminology Comparison}
\centering
\begin{tabular}{|l|l|}
\hline
Traditional & NGRM \\
\hline
job & job \\
job step & lightweight job (LWJ) \\
MPI job & lightweight job (LWJ) \\
reservation & job \\
\hline
\end{tabular}
\label{tab:tradterms}
\end{table}

In traditional terms, a user submits a request to a batch scheduler
for an allocation of computing resources.  This request is added to a
queue of other such requests.  When the scheduler grants a request, a
job is begun and set of resources are allocated to the job.  At that
point, one to many tasks are launched to do the work of the job.

Depending on the system, tasks are launched by a remote launcher such
as {\em mpirun} or {\em srun}.  Each set of tasks so launched are
known as a job step.  There can be one to many job steps active within
a job throughout the life of a job.  These can run sequentially or in
parallel, and can either run on dedicated or shared resources.  The
history of the job includes a detailed accounting of each job step
including the resources used and the duration of the job step.

By first approximation, the traditional job maps to an NGRM job.  The
job step maps to a lightweight job (LWJ).  Job scripts that invoke a
series of job steps will translate to a series of LWJ's in an NGRM
job.

Some job scripts launch multiple job steps in parallel (backgrounded)
and a resource manager will schedule the associated task launches in a
FIFO basis, based on the available resources and whether dedicated or
shared resources were requested.  As described above, NGRM's job
construct will include the scheduler service.  This provides the user
even more flexibility to schedule the order of the LWJ instantiations
and task placement on resources.

Traditional job and job step accounting statistics are commonly saved
to a database.  In NGRM, the LWJ's statistics will be reaped by the
parent job as described below.

Historically, batch schedulers provide a means to reserve a set of
resources for exclusive use by a user or group of users.  These
reservations are typically created in advance and are offered as
Dedidcated Application Times (DATs) for users' exclusive use.  In
NGRM, job reservations will take the form of a parent job (with a
future start time) under which user jobs will run.

%\subsection{Hierachical Job Model}
\subsection{High Level Design}

Given the multitude of challenges and our ambitious vision for
\ngrm\ (Section~\ref{sect:vision}), we have developed a novel
high level design for \ngrm\ that is based loosely upon the
divide and conquer algorithm through {\em job recursion}. That
is, in \ngrm\ every job itself is actually a full implementation
of the \ngrm\ system, and therefore new jobs can be submitted to a
local job in order to access the full power of \ngrm\ for managing
resources assigned to the job. In theory, this mechanism can be
used indefinitely such that the resources of an entire center
are {\em divided} naturally through normal workloads.  
%TODO: Recursive job model concept figure

When \ngrm\ initializes, there exists a single {\em root} or {\em
bootstrap} job which contains all the resources that are managed
by \ngrm. The root job is the only job which does not have a
parent job, i.e. it serves as the root of the heirarchical tree of
jobs (an analogy in the UNIX idiom would be that the root instance
is the {\tt init} process.) Instead of having a parent, the root
job instead gets data about resources, users, and configuration
from a global, peristent database, which serves as a configuration
repository in \ngrm. We could say that the configuration repository
is the parent of the root job.

The root job and its progeny all have the following features:

\begin{itemize}
\item{A job {\em owner}, and access lists of users able to run
      within the  current job and/or submit new jobs}
\item{A resource manager component configured with the list of
      resources allocated to the job, their topology and other information}
\item{A scheduler for accepting and scheduling new jobs}
\item{A job database for recording new, running, and completed child jobs}
\item{An ``isolated'' communications framework with {\em gateway} functionality
      for relaying messages to the parent}
\item{A distributed and featureful runtime environment capable of launching
      or assisting the launch of parallel applications and tools}
\item{An ``interactive'' node on which batch scripts and or user logins
       are contained}
\item{A resolvable domain name based on a unique job id}
\item{A local monitoring domain}
\end{itemize}

Since many of these components will be implemented via plugins or
have plugin capabilities, it is expected that several features will
be user-selectable during job submission. For example, the root
job may have a complex, distributed scheduler, whereas users may
want to choose between FIFO, backfill, or other simpler schedulers
depending on the in-job workload.

By default, only the owner of a job may submit new jobs or access
the runtime services of a running job.  While the ownership of a job
should not be changed, it will be possible for users to add other
users to the access list within their job, thus ``inviting'' the
submission of new jobs by others. The requirement for this feature
is clear when considering the root job, from which all other jobs
are spawned. This job will be owned by a privileged user such as
root, yet the system administrators will obviously want to open
up this root job for access to all users who should be able to
run jobs in the center.

When a job terminates, either due to a timelimit or the work
submitted for the job has completed, the job releases most resources
back to the parent job. The control functions of the job remain and
wait for an asynchronous {\em reap} operation from the parent. When
the job is reaped the parent job reads in all data from the child\'s
local job and resource database and instantiates that data in
its own databases. In this way, global information about jobs
percolates back up through the job heirarchy, eventually back to
the root job.  The root job then periodically updates the global,
persistent databases.

In the recursive job model, the so-called {\em base case} is a
single job in which a single parallel application is invoked. For
this case, a fully functional child job is not needed, and indeed
would be a waste of resources. For this purpose we introduce the
concept of a {\em lightweight job}(LWJ) which is submitted and
runs on the resources of the local job, but does not result in
a new invocation of all the features of a full job. Where
\ngrm\ jobs are like processes in a UNIX process tree, lightweight jobs
are like threads running within a process.

Lightweight jobs have access to a feature rich, distributed,
runtime environment with a shared key-value store, advanced
placement services, and a plugin interface that allows extension
of these services for unique requirements. This environment will
enable the quick deployment of advanced runtime features such as
fast parallel launch of MPI applications and easy tool integration.
Since LWJs use the same interface to job management as full
jobs, their existence, assigned resources, duration, etc.
will be recorded in the local job database for posterity.
In order to run a LWJ a user must be the owner of the current
job or on the runtime access list for the local job.
% or a parent? I can't remember what we decided here...


\subsection{Project Organization and Thrust Areas}
\label{sect:projorg}
To hasten the design, development, and delivery of the initial version
of \ngrm\, we define five relatively independent thrust areas.
Each thrust area will carry out the vision and high level design articulated
above with a focus on a particular subsystem or group of subsystems that
have a natural coupling.  The overall design of \ngrm\ will evolve and
require the whole team to weigh in on important changes, and the design
of interfaces between subsystems will require collaboration between
thrusts, but the work in each thrust can in large part progress
independently, building on the strength of the other thrusts.

Deliverables in each of the thrusts will be structured 
to leverage the framework approach to get the system up and running
early with simple plugins that are enhanced later.
Early prototypes will be used to accelerate the design of subsystem
interfaces and to obtain feedback from stakeholders and domain experts
as soon as possible.
Off-the-shelf components, external collaborations, and novel ideas from
the research and development communities will be leveraged where possible.
Thrust areas will adhere to a common set of project development practices
and standards (see Appendix~\ref{sect:process}).

Each of the thrust areas is briefly introduced below, then described in
more detail in the sections that follow.

\paragraph{Communication Framework}
\ngrm\ will devise a communication framework based on Internet Protocol
that supports our hierarchical job model, providing rich, scalable services
within a job and more limited communication bewteen jobs using
the job's {\em control node} as gateway.
The {\em comms toolkit} provides a security model, tools for
encoding and decoding messages, and tools for transporting messages between
nodes using various messaging patterns including PUB-SUB (with multicast),
RPC, and streaming.
A {\em comms message broker} tracks node liveness, provides services for
building fault-tolerant communication structures within the job,
and provides a multicast {\em scheduling trigger} used to synchronize
system overhead within the job to reduce noise.
A persistent {\em reduction network}
provides a reduction sieve capability for the job rooted at the control node.

\paragraph{Resource Management}
This thrust incorporates resource management and scheduling at the heart of 
the system.  A {\em resource description language} will be developed to
express resource requests and to describe resources in the resource inventory.
The {\em resource inventory} is a persistent database of resources managed
by the system, while the {\em resource database} is a per-job, read-write
cache of a portion of the resource inventory.
Similarly, a {\em job repository} and {\em job database} within each job
tracks information about jobs as they move through their life cycle,
and records a {\em provenance record} for each job, capturing pertinent
information about the job's runtime environment.
 The {\em scheduler} computes a schedule that matches available
resources to job requests.  

\paragraph{Monitoring}
Resource managers must monitor resource health to avoid scheduling
work on broken hardware.  \ngrm\ provides a comprehensive monitoring
environment including a {\em plugin framework} with data reduction
capability, synchronization of monitoring overhead across the job,
and the ability to tune the monitoring period and change the plugin
stack on a per-job basis.
A {\em fault notification system} enables system software, runtimes, and
applications to share fault information as a basis for building fault-tolerant
workloads and for recording interesting events with the job record.
\ngrm\ monitoring interfaces with an {\em external log database} intended
to support post-mortem analysis, and to
an {\em external enterprise monitoring system} such as might be used in
a site operations center.

\paragraph{Workload Runtime and Placement}
FIXME

\paragraph{I/O Management}
FIXME

\include{framework}

\include{resmgmt}

\include{monitor}

\include{io}

\include{runtime}

\include{project}

\appendix

\include{reqs}

\include{process}

\bibliographystyle{abbrv}
\bibliography{../bib/project,../bib/rfc}

\end{document}
