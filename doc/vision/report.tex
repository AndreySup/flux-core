\documentclass{article}

\usepackage{verbatim}
\usepackage{calc}
\usepackage{epsfig}
\usepackage{url}
\usepackage{draftcopy}
\usepackage{longtable}
\input{pstricks}
\usepackage{multirow}

\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{(\paperwidth-\textwidth)/2 - 1in}
\setlength{\topmargin}{(\paperheight-\textheight -\headheight-\headsep-\footskip)/2 - 1in + .5in }

\newif\ifwbs
\wbsfalse

\newcommand{\ngrm}{NGRM}
\newcommand{\ngrmfull}{Next Generation Resource Manager}
\newcommand{\ngjs}{NGRM Job Scheduler}
%\newcommand{\zMQ}{${\varnothing}$MQ}
\newcommand{\zMQ}{\O{}MQ}
\newcommand{\slurm}{Slurm}
\newcommand{\moab}{Moab}

%\includeonly{monitor}

\begin{document}

\title{Vision and Plan for a \ngrmfull}
\author{\
Dong H. Ahn, ahn1@llnl.gov\\
Jim Garlick, garlick@llnl.gov\\
Mark Grondona, mgrondona@llnl.gov\\
Don Lipari, lipari@llnl.gov}

%\date{Nov 6, 2012}

\maketitle

\section{Overview}
\label{sect:overview}
Resource Management (RM) software is critical for High Performance Computing
(HPC). It is the centerpiece that allows efficient execution of HPC applications
while providing an HPC center with the main means to maximize 
the utilization of its computing resources. However, several growing trends make
even the best-in-breed RM software largely ineffective. As numbers and
types of compute cores of HPC systems continue to grow, key RM challenges
associated only with today's {\em capability-class} machines are 
becoming increasingly pervasive for {\em all} computing resources including 
commodity Linux clusters. The challenges include having to provide 
extreme scalability, low noise, fault tolerance, and heterogeneity management
while under a strict power budget.

In addition, greater difficulties in code development on larger systems have
begun to impose far more complex requirements on the RM. For example,
without adequate RM support, debugging, tuning, testing and verification
of the applications have become too difficult and time-consuming for end-users.
The next-generation code development environments require the RM to provide
effective mechanisms to support the reproducible results of program execution, to
provide accurate correlations between user-level errors and system-level
events, and to integrate and accelerate a rich set of scalable tools.

Further, a greater interplay among various classes of clusters across
the entire center makes the current practice of single-cluster scheduling
far less appealing. An application running on a compute cluster heavily
utilizes site-wide shared resources such as I/O and visualization clusters.
Thus, avoiding any significant site-wide bottleneck requires the RM
to schedule the job to all dependent resources together. In short, without
the RM that can effectively address all of these challenges, it has become apparent
that HPC centers will suffer a significant loss in both user productivity
and efficient uses of next-generation computing resources.

\slurm\cite{SlurmDesign} is arguably the
best-in-breed, open-sourced RM designed for commodity Linux clusters.
Livermore Computing (LC) at Lawrence Livermore National Laboratory (LLNL)
designed and led its implementation in 2002 and since then has facilitated
its wide adoption outside of LLNL.
\slurm's original design was for a 1-2K node Linux cluster with a single
interconnect domain and bulk-synchronous workload based on MPICH.
In the last decade, \slurm\ has been adapted
to act as a scheduling and job submission layer on top of proprietary
RM software on IBM Blue Gene and Cray systems,
to support other runtimes quite different from MPICH,
to run (natively) on Linux clusters pushing beyond the designed node count,
and to be tied into a \moab\cite{MOAB:online} grid.
These adaptations have accrued without fundamentally changing the
core \slurm\ design, and as a result \slurm\ is increasingly
unable to adapt to emerging requirements without compromising
stability and code maintainability.

Our response to this critical need is the \ngrmfull\ (\ngrm ), an RM software
{\em framework} that can solve the key emerging challenges 
in a simple, extensible, distributed and autonomous fashion.
It aims at managing the whole center as one common pool of {\em diverse} 
resources. Hence, scheduling decisions will be 
far more efficient as well as flexible to accommodate 
emerging constraints such as a strict power bound. 
Further, \ngrm\ integrates
system monitoring, system administration, lightweight
virtualization, and distributed tool communication capabilities
that are currently provided by disjoint and often overlapping software.
Integrations of these facilities within the common framework designed from
the ground up for scalability, security, and fault tolerance will result
in a more efficient and capable system.

We realize that \ngrm's problem space is huge and yet we must address a wide range 
of challenges in relatively short order with limited resources. Thus, we 
organize our project plan around four research and development thrust areas 
and seek to advance them systematically.
These areas are called:
{\bf{\em Resource Management}},
{\bf{\em Monitoring and Data Management}},
{\bf{\em Workload Runtime And Placement}}, and
{\bf{\em Communications Framework}}.
They are relatively independent but can significantly build on the
strength of one another through well-known interfaces and common design
and development principles.
Thus, reaching all major milestones of these thrust areas 
will represent the completion of the first round of \ngrm\ development. 

Overall, \ngrm\ will significantly improve operational efficiency for
scientific application development and execution, and further for computing
resources of the entire HPC center.  It will also provide
a foundation for further extension and customization, allowing agile responses
to site-specific scheduling issues. Perhaps more importantly, \ngrm\
positions us to cope with a blend of interrelated, diverse
extreme-scale computing resources, the landscape of high-end HPC centers
in just a few years down the road

The rest of the paper is organized as the following.
Section~\ref{sect:vision} presents \ngrm's vision and 
its new capabilities in more detail. Section~\ref{sect:projorg}  
describes the high-level goals of the four thrust areas and
explains relations among the areas. Next sections then 
go over and detail each of these areas including 
its work breakdown structure (WBS) and associated work items.
Finally, Section~\ref{sect:delivery} defines
our schedules for milestones and deliverables.


\section{Vision and New Capabilities}
\label{sect:vision}

The vision of \ngrm\ is to create a scalable RM software system that 
drastically improves operational efficiency and user productivity 
for workloads on {\em capacity-class} compute systems.
With a trend towards ever-growing numbers and types of compute cores, however,
this system class has been subject to the challenges that
today's {\em capability-class} machines face. 
These challenges include having to provide extreme scalability, low noise, 
fault tolerance, and heterogeneity management while under a strict power budget.
Worse, the workloads themselves are also becoming increasingly diverse, 
dynamic and large. Thus, fully realizing our vision through these challenges requires
a {\em paradigm shift} in how the RM should manage and model its resources.

In the new paradigm, the RM must be capable of imposing highly complex resource bounds
to guarantee the highest operational efficiency at any level
across the computing facility, while at the same time enabling most efficient execution
and scheduling of the workloads within these bounds.
Thus, the RM must manage the entire facility as one
common pool of resources. The ability to see a broader spectrum of resources 
and their various constraints can then lead to most efficient scheduling strategies
and execution environments. The new paradigm also demands that the RM model
various types of resources and their relationships beyond the traditional resource model:
i.e., a simple collection of compute nodes.
The rich resource model will allow the RM to allocate computing resources
tailored to the disparate limiting factors of our applications: e.g.,
an application may be compute-bound while others are I/O-bound or power-bound.
Under the new paradigm, the resource allocations must also be elastic. 
An application may have different phases with disparate performance-limiting factors;
it must be able to grow and shrink its resource allocation dynamically. 
The global resource view, rich resource model and elasticity represent
the fundamental characteristics of the RM under the new paradigm.

Further, we envision that the RM must provide the central framework to integrate
other relevant software including system monitoring and administration, 
lightweight virtualization, and scalable tool communication. 
The integration will facilitate a higher level of leverage 
among these essential computing elements, and this will lead to significantly higher
productivity for both end users and system adminstrators.  
In addition, as these capabilities are currently provided through disjoint
and often overlapping software, the integration will substantially reduce
the costs needed for developing and maintaining individual software.  
In the following, we further detail the key ideas and new capabilities of \ngrm.


\subsection{Center as a Cluster}

Unlike the traditional approach of running an RM instance on each separately
managed cluster, and tying clusters together with grid software almost was
an afterthought. Instead, \ngrm\ will be designed from the beginning to manage the
center as one pool of resources. There will be a single site-wide 
system image from the perspective of users as well as system administrators.
With this approach, filesystem servers such as Lustre clusters, 
and visualization and serial batch systems can be aggregated
with compute clusters into one management domain. Hence,
our RM can make better global scheduling decisions. 
In addtion, dedicated interactive nodes will be unnecessary in the new system, as it
will be possible to allocate interactive environments on demand from the
global pool of resources.

With integrated, center-wide monitoring, it becomes easier
for our system to associate center Reliability, Availability and Serviceability~(RAS) 
events such as a global filesystem
failure with an affected job and to make that information part of
the job's data {\em provenance} record. When the RM obtains a global view of
resources including shared persistent storage, it becomes possible to
consider the scheduling of I/O along with computation. In combination with
I/O forwarding software, our RM could easily set up unique I/O forwarding
topologies for each job.

System management overhead that currently scales with the number of clusters
could be streamlined by aggregating per-cluster services into center-wide
services. For example, at LLNL, each cluster has a pair of management
nodes that are bootstrapped from scratch with an OS installation from DVD,
and when system software is updated on that cluster, a downtime is
scheduled during which the workload sits idle and the responsible system
administrator performs a sequence of steps. Under the new RM, the
number of systems that have to be brought up from scratch will be
greatly reduced, and administrator-orchestrated downtimes for software
updates could be eliminated. 


\subsection{Zero Downtime}

The impact of downtime becomes greater with a higher degree of integration
of systems and services. Thus, \ngrm\ must be tolerant
of hardware and software faults and failures.
It will be designed to have no single point of failure and support 
version interoperability that allows {\em live} software upgrades.
The technique will faciliate a rolling update across the center 
without impacting overall center availability and/or running workloads.

\ngrm\ will support the notion of lightweight containers 
and private filesystem namespaces for jobs, achieving 
a higher degree of isolation between system software and user-visible 
software. For example, the RM and the Operating System (OS)
kernel might see one version of the root filesystem, while an application
might see another. The decoupling and isolation will be the key to the 
much desired ability to upgrade the software levels in the machine's
root filesystem without affecting any of the
libraries that an application might be using, and vice-versa. 
Similarly, one can upgrade the system OS image at idle points between jobs,
with user-level software remaining unchanged, or vice versa. The separation of
concerns gives more flexibility to the organization in determining software
update policy and in fact could allow users or code development teams to
control the software levels affecting their application, independent of
other applications.

\subsection{Low Noise}

As the number of processes in a parallel application increases, 
their executions are affected to varying degrees by OS
scheduling jitter, depending on their communication patterns. Minimizing
the user-space system software contribution to OS jitter will be one 
of our primary design goals.  \ngrm\ will supplant the independent monitoring, 
remote shell, and cron
services that contribute to noise today. The integrated services will
allow users to dial up or down the verbosity and frequency of monitoring,
depending on their debug/monitoring needs versus their application's noise
sensitivity. Cron (periodic housekeeping) jobs and rsh (remote command
executions) can be performed through the RM to minimize their impact,
such as running them between jobs or synchronized across jobs. 
\ngrm\ will also be flexible enough to allow implementation of other
strategies for reducing the impact of noise, such as scheduling all
non-application activity to a configurable {\em noise core}.


\subsection{Data Provenance and Reproducibility}

As simulation plays an increasingly central role in scientific
investigation, reproducibility of results is more important than ever
before. A result should be accompanied by a data provenance record that
can be used by others to recreate the inputs and conditions that led to
that result. It should also record unusual system activities 
such as RAS events that might help in a post-mortem analysis 
when expected results are not obtained.
The new RM will produce such a record for every job.
Long running parameter studies or uncertainty quantification runs
require stability for long periods of time. Private filesystem
namespaces, as described above, enable applications to lock down their
environment including dependent shared libraries so that the effects from
system updates on the application are minimized.

\subsection{Rich Resource Model}

Traditional RM's in HPC systems have focused on node- and/or CPU-centric
scheduling. With the advent of hybrid compute systems utilizing specialized,
heterogeneous resources, this model has become less effective. Rather than
perpetuating a node-centric resource model with support for generic
resources as an add-on, \ngrm\ will strive to support for heterogeneity
from the start.
Thus, the idea of a resource will be kept as generic as
possible. That will not only faciliate simpler handling of generic resources,
but also enable future expansion to resource types that have yet to
be conceived. New resource types will provided through configuration changes
and/or simple extensions that inherit their attributes from base types 
to maximize reuse and foment collaboration.  Resource topology will be encoded via
configuration, and resources will also be allowed to have tags or labels
to which resource requests may refer. 
A generic resource query language will be developed to allow flexible
specification of resource requirements.

\subsection{Extensibility}

Plugins, when designed properly, minimize the knowledge of internals
needed to extend and modify a system. They provide a mechanism for
sites to customize the system for their particular needs. To the extent
possible, \ngrm\ will be modularized so various subsystems can be replaced
or extended with plugins.
In addition, \ngrm\ will be designed to be self-hosting: it will
be capable of launching a standalone copy of itself as a job. This makes
it possible for developers or a QA team to test new versions of the
RM without requiring dedicated access to a cluster.
In combination, these features lower the barrier of entry into the
community of developers supporting and extending the RM.

\subsection{Security}

Distributed components of \ngrm\ will support privacy and integrity on
the network to limit vulnerability to attacks involving physical access
to a system or its networks.
Lightweight containers will tightly control the access that
applications have to system resources. For example, it is possible to
run the job in its own network namespace such that direct access to the
system management network is unavailable, or limited by node firewall
rules. \ngrm\ could also use the private network namespace to isolate
jobs from each other on virtual private networks.  
Containers with private filesystem namespaces can limit visibility of
filesystems to jobs based on the site policy. 
In combination with I/O
forwarding software, \ngrm\ could squash all access to filesystems to
the user id of the job so privilege escalation within the job such as
escaping a container or obtaining root within the container would not
change the user's identity when accessing the filesystem.

\subsection{Research and Tool Friendly}

Components of \ngrm\ will be designed with the ability to capture and
publish sanitized system data at all levels to facilitate the use of
this data in current and future research.  The plugin and self-hosting
features of the system will encourage experimentation and
facilitate research activities by allowing new and experimental
versions of the software to be run within a production instance.
One of our design goals is also to facilitate the
development and use of scalable runtime code development tools 
on jobs on next-generation computing systems. This will not only serve 
to lower the barrier to create and deploy useful tools, but will also reduce the
profusion of one-off software systems in use on production systems.


\section{Major Factors in Our Design Space}

TO BE DRAFTED. WE WANT TO NAME CHALLENGES AND FACTORS IN THIS SECTION SO THAT WE CAN MAP THEM TO OUR RM MODELS AND DESIGN SOMEWHERE DOWN THE ROAD. 

\begin{itemize}
\item{Scalability in concurrency} 
\item{Scalability in number of jobs} 
\item{Workload Diversity}
\item{Dynamic workload}
\item{Emerging resource types}  
\item{Scheduleabilty}  
\item{Reproducibility}
\item{Data provenance}
\item{Noise control}
\item{Productivity tools}  
\item{Integration risk}
\item{Expensive downtime}
\item{Security}
\end{itemize}


%Multi-dimensional scalability problem: When one wants to design a resource management software system that can manages the resources that are across the entire computing facility, extreme care must be taken for scalability. Being able to run programs at larger scales is one dimension; and avoiding some central location with large number of jobs is another dimention to the scalabilty problem. 
%
%
 





\section{Approach}
\subsection{Terminology}

\paragraph{comms session}
An established communication association among a set of nodes that
enables secure, scalable, elastic, and fault-resilient communication
services for an \ngrm\ {\bf instance}.
A comms session is identified by its private DNS name, e.g. s1.\ngrm.
or s1.s1.\ngrm.

\paragraph{confinement}
The enforcement of resource allocations such that an {\bf instance} cannot
consume more than was allocated.  Confinement of an instance to assigned
resources is the responsiblility of the parent and, in the case of inherited
confinment, grandparent instance(s).

\paragraph{control node}
A distinguished node within a {\bf comms session} which holds the master
{\em comms state}, is the root of the session's aggregation/reduction tree,
and which performs gateway functions with the parent session.

\paragraph{instance}
An independent set of resource manager services configured to manage
a set of resources.
Instances are created dynamically and recursively.
We refer to the bootstrap {\em root instance} which contains all resources,
and speak of {\em parent}, {\em child}, and {\em sibling} relations between
instances.
An instance is identified by its {\bf comms session} name,
e.g. s1.\ngrm.  or s1.s1.\ngrm.

\paragraph{job}
A time-bounded allocation of resources.  A job is submitted to a
particular {\bf instance}.  When it executes, a child instance is
created to contain the job.  As a special case the {\em root job} 
runs perpetually without a time bound.
An job is identified by its {\bf instance} name,
e.g. s1.\ngrm.  or s1.s1.\ngrm.

\paragraph{lightweight job} (LWJ) 
A {\bf job} submitted and recorded in the current {\bf instance}
which does not result in a new instance.

%\subsection{Hierachical Job Model}
\subsection{High Level Design}

Given the multitude of challenges and our ambitious vision for
\ngrm\ (Section~\ref{sect:vision}), we have developed a novel
high level design for \ngrm\ that is based loosely upon the
divide and conquer algorithm through {\em job recursion}. That
is, in \ngrm\ every job itself is actually a full implementation
of the \ngrm\ system, and therefore new jobs can be submitted to a
local job in order to access the full power of \ngrm\ for managing
resources assigned to the job. In theory, this mechanism can be
used indefinitely such that the resources of an entire center
are {\em divided} naturally through normal workloads.  
%TODO: Recursive job model concept figure

When \ngrm\ initializes, there exists a single {\em root} or {\em
bootstrap} job which contains all the resources that are managed
by \ngrm. The root job is the only job which does not have a
parent job, i.e. it serves as the root of the heirarchical tree of
jobs (an analogy in the UNIX idiom would be that the root instance
is the {\tt init} process.) Instead of having a parent, the root
job instead gets data about resources, users, and configuration
from a global, peristent database, which serves as a configuration
repository in \ngrm. We could say that the configuration repository
is the parent of the root job.

The root job and its progeny all have the following features:

\begin{itemize}
\item{A job {\em owner}, and access lists of users able to run
      within the  current job and/or submit new jobs}
\item{A resource manager component configured with the list of
      resources allocated to the job, their topology and other information}
\item{A scheduler for accepting and scheduling new jobs}
\item{A job database for recording new, running, and completed child jobs}
\item{An ``isolated'' communications framework with {\em gateway} functionality
      for relaying messages to the parent}
\item{A distributed and featureful runtime environment capable of launching
      or assisting the launch of parallel applications and tools}
\item{An ``interactive'' node on which batch scripts and or user logins
       are contained}
\item{A resolvable domain name based on a unique job id}
\item{A local monitoring domain}
\end{itemize}

Since many of these components will be implemented via plugins or
have plugin capabilities, it is expected that several features will
be user-selectable during job submission. For example, the root
job may have a complex, distributed scheduler, whereas users may
want to choose between FIFO, backfill, or other simpler schedulers
depending on the in-job workload.

By default, only the owner of a job may submit new jobs or access
the runtime services of a running job.  While the ownership of a job
should not be changed, it will be possible for users to add other
users to the access list within their job, thus ``inviting'' the
submission of new jobs by others. The requirement for this feature
is clear when considering the root job, from which all other jobs
are spawned. This job will be owned by a privileged user such as
root, yet the system administrators will obviously want to open
up this root job for access to all users who should be able to
run jobs in the center.

When a job terminates, either due to a timelimit or the work
submitted for the job has completed, the job releases most resources
back to the parent job. The control functions of the job remain and
wait for an asynchronous {\em reap} operation from the parent. When
the job is reaped the parent job reads in all data from the child\'s
local job and resource database and instantiates that data in
its own databases. In this way, global information about jobs
percolates back up through the job heirarchy, eventually back to
the root job.  The root job then periodically updates the global,
persistent databases.

In the recursive job model, the so-called {\em base case} is a
single job in which a single parallel application is invoked. For
this case, a fully functional child job is not needed, and indeed
would be a waste of resources. For this purpose we introduce the
concept of a {\em lightweight job}(LWJ) which is submitted and
runs on the resources of the local job, but does not result in
a new invocation of all the features of a full job. Where
\ngrm\ jobs are like processes in a UNIX process tree, lightweight jobs
are like threads running within a process.

Lightweight jobs have access to a feature rich, distributed,
runtime environment with a shared key-value store, advanced
placement services, and a plugin interface that allows extension
of these services for unique requirements. This environment will
enable the quick deployment of advanced runtime features such as
fast parallel launch of MPI applications and easy tool integration.
Since LWJs use the same interface to job management as full
jobs, their existence, assigned resources, duration, etc.
will be recorded in the local job database for posterity.
In order to run a LWJ a user must be the owner of the current
job or on the runtime access list for the local job.
% or a parent? I can't remember what we decided here...


\section{Project Organization and Thrust Areas}
\label{sect:projorg}
To hasten the design, development, and delivery of the initial version
of \ngrm\, we define four thrust areas that are relatively independent
and build on the strength of one another through well-known interfaces
and common design and development principles (see Appendix~\ref{sect:process}).

Creating \ngrm\ will require substantial collaborative efforts with
experts from several domains.
The software architecture must bolster rapid progress in all of our thrust
areas and facilitate mechanisms by which to leverage external innovation.

FIXME: describe functional architecture, leading into thrust areas

The four thrust areas are
Communication Framework,
Resource Management,
Monitoring and Data Management,
and Workload Runtime and Placement.

\subsection{Communication Framework}
\ngrm\ will devise a secure and fault-tolerant backbone communication
infrastructure that can provide requisite scalability and flexible
capabilities for all \ngrm\ operations as well as the entire HPC
software ecosystem it embraces.

\subsection{Resource Management}

%This paragraph needs work to reflect revised thrust areas
\ngrm\ will investigate the notion of generalized resources so that
it can efficiently schedule a job to the resources that are beyond the
traditional definition of computing resources. An abstraction for
generalized resources will effectively handle a wide variety of computing
resources, from traditional compute cores and memory, to heterogeneous
computing elements, to consumable resources like power.
The highest-level of abstraction will allow the management of an
entire HPC center as one pool of resources.

\subsection{Monitoring and Data Management}

\ngrm\ will investigate methods of transparently generating provenance
information for each job, and enabling this information to be combined
with application-level provenance and user annotations to form
comprehensive documentation for simulation results.

\ngrm\ will research tools and mechanisms for encapsulating application
code and its software dependencies in immutable containers such that
this software can be held constant from run to run, aggressively
cached during job launch, and easily ported to other sites.

\ngrm\ will research scalable and extensible logging and monitoring
frameworks that have a tunable impact on system scheduling jitter and
can deliver high-volume, user-driven debug information to filters within
the job and/or stable storage for post-mortem analysis.

\ngrm\ will research scalable database technology to store
logs, fault information, and provenance data, which can be used
to correlate information from disparate sources for post-mortem debugging,
documenting provenance, and developing RAS metrics.

Finally, \ngrm\ will investigate fault notification mechanisms
to allow applications and system software to publish
faults which can trigger log dumps or a recovery response.

%\ngrm\ will create and advance provisioning systems that allow
%user control over file systems portion of the runtime environment
%for better reproducibility of simulation results.

\subsection{Workload Runtime}

\ngrm\ will develop and use a software design methodology to
guarantee that all software components our thrust areas produce
abide by a common set of desired attributes, including low noise,
high security, and fault containment with no single point of failure.

\include{framework}

\include{resmgmt}

\include{monitor}

\include{io}

\include{runtime}

\include{project}

\appendix

\include{reqs}

\include{process}

\bibliographystyle{abbrv}
\bibliography{../bib/project,../bib/rfc}

\end{document}
