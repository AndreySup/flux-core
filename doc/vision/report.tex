\documentclass[10pt]{article}

\usepackage{verbatim}
\usepackage{calc}
\usepackage{epsfig}
\usepackage{url}
\usepackage{longtable}
\input{pstricks}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{type1cm}
\usepackage{eso-pic}
\usepackage{color}
\usepackage{cite}
\usepackage{listings}

% draft watermark begin
\makeatletter
\AddToShipoutPicture{
	\setlength{\@tempdimb}{.5\paperwidth}
	\setlength{\@tempdimc}{.5\paperheight}
	\setlength{\unitlength}{1pt}
	\put(\strip@pt\@tempdimb,\strip@pt\@tempdimc){
		\makebox(0,0){\rotatebox{55}{\textcolor[gray]{0.85}
			{\fontsize{5cm}{5cm}\selectfont{DRAFT}}}}
	}
}
\makeatother
% draft watermark end

\graphicspath{ {../fig/} }

\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{(\paperwidth-\textwidth)/2 - 1in}
\setlength{\topmargin}{(\paperheight-\textheight -\headheight-\headsep-\footskip)/2 - 1.2in }

% enable/disable WBS tables
\newif\ifwbs
\wbsfalse
%\wbstrue

% enable/disable margin comments
\newif\ifcomments
%\commentsfalse
\commentstrue

\newcommand{\ngrm}{NGRM}
\newcommand{\ngrmfull}{Next Generation Resource Manager}
\newcommand{\ngjs}{NGRM Job Scheduler}
%\newcommand{\zMQ}{${\varnothing}$MQ}
\newcommand{\zMQ}{\O{}MQ}
\newcommand{\slurm}{Slurm}
\newcommand{\moab}{Moab}

\DeclareRobustCommand{\orderof}{\ensuremath{\mathcal{O}}}

%\includeonly{monitor}

\begin{document}

\title{Vision and Plan for a \ngrmfull}
\author{\
Dong H. Ahn, ahn1@llnl.gov\\
Jim Garlick, garlick@llnl.gov\\
Mark Grondona, mgrondona@llnl.gov\\
Don Lipari, lipari@llnl.gov}

%\date{Nov 6, 2012}

\maketitle

\section{Overview}
\label{sect:overview}
Resource Management (RM) software is critical for High Performance Computing
(HPC). It is the centerpiece that allows efficient execution of HPC applications
while providing an HPC center with the main means to maximize 
the utilization of its computing resources. However, several growing trends make
even the best-in-breed RM software largely ineffective. As numbers and
types of compute cores of HPC systems continue to grow, key RM challenges
associated only with today's {\em capability-class} machines are 
becoming increasingly pervasive for {\em all} computing resources including 
commodity Linux clusters. The challenges include having to provide 
extreme scalability, low noise, fault tolerance, and heterogeneity management
while under a strict power budget.

\ifcomments
\marginpar{\tiny {\bf ned-review:} Citation needed.
The shortcomings identified in this section form the core justification for
why NGRM is needed, so you should be prepared to back them up with references.}
\fi
In addition, greater difficulties in code development on larger systems have
begun to impose far more complex requirements on the RM. For example,
without adequate RM support, debugging, tuning, testing and verification
of the applications have become too difficult and time-consuming for end-users.
The next-generation code development environments require the RM to provide
effective mechanisms to support the reproducible results of program execution, to
provide accurate correlations between user-level errors and system-level
events, and to integrate and accelerate a rich set of scalable tools.

Further, a greater interplay among various classes of clusters across
the entire computing facility makes the current paradigm of single-cluster scheduling
largely ineffective. An application running on a compute cluster heavily
utilizes site-wide shared resources such as I/O and visualization clusters.
Thus, avoiding any significant site-wide bottleneck requires the RM
to schedule the job to all dependent resources together. In short, without
the RM that can effectively address all of these challenges, it has become apparent
that HPC centers will suffer a significant loss in both user productivity
and efficient uses of next-generation computing resources.

\ifcomments
\marginpar{\tiny {\bf ned-review:} Citation needed.
As above, we need more than anecdotal evidence that Slurm can't meet future
requirements.}
\fi
\slurm~\cite{SlurmDesign} is arguably the
best-in-breed, open-sourced RM designed for commodity Linux clusters.
Livermore Computing (LC) at Lawrence Livermore National Laboratory (LLNL)
designed and led its implementation in 2002 and since then has facilitated
broad adoption outside of LLNL.
\slurm's original design was for a moderate-size Linux cluster with
$\orderof\left(2K\right)$
compute nodes connected in a single interconnect domain and MPICH-based, 
bulk-synchronous applications.
In the last decade, beginning with multi-core support added by
Hewlett-Packard\cite{DBLP:conf/jsspp/BalleP07},
\slurm\ has been modified to meet emerging challenges
that stray outside its core design.
For example, recently
it has been adapted to act as a scheduling and job submission layer
on top of proprietary RM software on IBM Blue Gene and Cray systems,
to support new run-times such as OpenMPI and MapReduce\cite{SlurmMR},
to implement hierarchical communication to increase scalability,
and to be tied into a \moab~\cite{MOAB} grid.
These adaptations have accrued without fundamentally changing the
core paradigm and design of \slurm, which is that of a monolithic, centralized
controller with compute nodes as the main, scheduled resource type.
As a result the \slurm\ implementation of these add-ons is less functional
and effective than it could be with a redesign,
and the \slurm\ code base grows increasingly unmaintainable, burdened
as it is with such after-thoughts.

Our response to this critical need is the \ngrmfull\ (\ngrm ), an RM software
{\em framework} that can solve the key emerging challenges 
in a simple, extensible, distributed and autonomous fashion.
It aims at managing the whole center as one common pool of {\em diverse} 
resources. Hence, scheduling decisions will be 
far more efficient as well as flexible to accommodate 
emerging constraints such as a strict power bound. 
Further, \ngrm\ integrates
system monitoring, system administration, lightweight
virtualization, and distributed tool communication capabilities
that are currently provided by disjoint and often overlapping software.
Integration of these facilities within the common framework designed from
the ground up for scalability, security, and fault tolerance will result
in a more efficient and capable system.

\ifcomments
\marginpar{\tiny {\bf ned-review:}
What about the security model--should it be its own thrust area?  It seems to me
a  robust security model will need to be incorporated from the ground up, so it
should be one of the first things we work on.
{\bf jg:} Security model is developed in comms thrust.}
\fi
We realize that \ngrm\ represents a paradigm shift for HPC resource management, 
and yet we must address a wide range of challenges in relatively short order
with limited resources. Thus, we 
organize our project plan around four research and development thrust areas 
and seek to advance them systematically.
These areas are called:
{\em Communications Framework},
{\em Resource Management},
{\em Monitoring}, and
{\em Workload Runtime And Placement}.
They are relatively independent but can significantly build on the
strength of one another through well-known interfaces and common design
and development principles.
Thus, reaching all major milestones of these thrust areas 
will represent the completion of the first round of \ngrm\ development. 

Overall, \ngrm\ will significantly improve operational efficiency for
scientific application development and execution, and further for computing
resources of the entire HPC center.  It will also provide
a foundation for further extension and customization, allowing agile responses
to site-specific scheduling issues. Perhaps more importantly, \ngrm\
positions us to cope with a blend of interrelated, diverse
extreme-scale computing resources, the landscape of high-end HPC centers
in just a few years down the road.

The rest of the paper is organized as follows.
Section~\ref{sect:vision} presents \ngrm's vision and 
its new capabilities in more detail. 
In Section~\ref{sect:designspace}, we discuss 
the key software design challenges and the conceptual models
that embody the new paradigm while addressing these design challenges. 
Section~\ref{sect:ngrm} then describes our software design
and also highlights the four research and development thrust areas
and their relations. 
The following sections then go over and detail each of these areas including 
its work breakdown structure (WBS) and associated work items.
Finally, Section~\ref{sect:delivery} defines
our schedules for milestones and deliverables.


\section{Vision and New Capabilities}
\label{sect:vision}

The vision of \ngrm\ is to create a scalable RM software system that 
drastically improves operational efficiency and user productivity 
for workloads on {\em capacity-class} compute systems.
With a trend towards ever-growing numbers and types of compute cores, however,
this system class has been subject to the challenges that
today's {\em capability-class} machines have been facing. 
These challenges include having to provide extreme scalability, low noise, 
fault tolerance, and heterogeneity management while under a strict power budget.
Worse, the workloads themselves are also becoming increasingly diverse, 
dynamic, and large. Thus, fully realizing our vision through these challenges requires
a {\em paradigm shift} in how the RM should manage, model, schedule,
and allocate its resources.

In the new paradigm, the RM must be capable of imposing highly complex resource bounds
to guarantee the highest operational efficiency at any level
across the computing facility, while at the same time enabling most efficient execution
and scheduling of the workloads within these bounds.
Thus, the RM must manage the entire facility as one
common pool of resources. The ability to see a broader spectrum of resources 
and their various constraints can then lead to most efficient scheduling strategies
and execution environments. Further, the same ability will ease 
efforts to diagnose errors for both end users and support staff
by associating jobs with other facility-wide events. 
The new paradigm also demands that the RM model
various types of resources and their relationships beyond the traditional resource representation:
i.e., a simple collection of compute nodes.
The rich resource model will allow the RM to allocate computing resources
tailored to the disparate limiting factors of our applications: e.g.,
an application may be compute-bound while others are I/O-bound or power-bound.
Under the new paradigm, the resource allocations must also be elastic. 
An application may have different phases with disparate performance-limiting factors;
it must be able to grow and shrink its resource allocation dynamically. 
The global resource view, rich resource model, and elasticity represent
the fundamental characteristics of the new resource management paradigm.

Further, the new paradigm must provide a central framework to integrate
other relevant software. The software components should include   
system monitoring and administration, lightweight virtualization, 
and scalable tool communication. The integration will 
facilitate a higher level of leverage among these essential computing elements, 
and this will lead to significantly higher productivity 
for both end users and system administrators.  
In addition, as these capabilities are currently provided through disjoint
and often overlapping software, the integration will substantially reduce
the costs needed for developing and maintaining individual software.  
In the following, we further detail the key ideas and new capabilities 
under the new paradigm.


\paragraph{Center as a Cluster:}
Unlike the traditional paradigm of running an RM instance on each separately
managed cluster, 
%and tying clusters together with grid software almost was
%an afterthought. 
the new paradigm must manage the
entire computing facility as one pool of resources. There must be a single site-wide 
system image from the perspective of users as well as system administrators.
With this approach, file system servers such as Lustre clusters, 
and visualization and serial batch systems can be aggregated
with compute clusters into one management domain. Hence,
our RM can make better global scheduling decisions. 

With integrated, site-wide monitoring, it becomes easier
for the new paradigm to associate global Reliability, Availability and Serviceability~(RAS) 
events such as a global file system
failure with an affected job and to make that information part of
the job's data {\em provenance} record. When the RM obtains a global view of
resources including shared persistent storage, it becomes possible to
consider the scheduling of I/O along with computation. In combination with
I/O forwarding software, our RM could easily set up unique I/O forwarding
topologies for each job.

This paradigm also has many possible advantages for system administration.
The resource inventory for the center is managed from a central point
and contains details that can drive center-wide configuration management.
A {\em cluster} is diminished as a primary, user-facing data center entity
and instead can be viewed as an arbitrary collection of resources, part of
a larger system, that happens to be attached to a single interconect domain
or that have other similar characteristics.
A cluster downtime, formerly viewed as a period of unavailability,
can be viewed in the new paradigm as a period of degraded performance.
Through lightweight virtualization, users obtain a degree of independence
from system software updates, which in some cases can quietly roll out
across the center between jobs with minimal impact.
As the new paradigm embraces heterogeneity within the larger system,
new resources can be purchased and added on, as dictated by demand,
without the need to build a standalone cluster entity, separately named
and managed, for every new type of hardware introduced.
In short, system administration activities can take place in a more
centralized, less visible manner such that they are no longer perceived
by users as at odds with their productivity.

\paragraph{Diverse Compute Resources:}
The traditional paradigm has solely focused on node- and/or CPU-centric
scheduling. With the advent of hybrid compute systems utilizing specialized,
heterogeneous resources and also of other bounding resources like power, 
this simple resource model has become largely ineffective. 
Rather than
perpetuating a node-centric resource model with support for other
resources as an add-on, the new paradigm must embrace the concept of generalized resources: 
the idea of a resource is kept as generic as possible. This will not only facilitate 
simpler handling of diverse resources, but also enable future expansion to resource types that have yet to
be conceived. New resource types can be provided through configuration changes
and/or simple extensions that inherit their attributes from base types 
to maximize reuse and foment collaboration. Various resource topologies
can be encoded via configuration, and resources can also be given tags or labels
to which resource requests may refer. 
The new paradigm must also provide a generic resource query language to allow flexible
specification of resource requirements.


\paragraph{Data Provenance and Reproducibility:}
As simulation plays an increasingly central role in scientific
investigation, reproducibility of results is more important than ever
before. A result should be accompanied by a data provenance record that
can be used by others to reconstruct the inputs and conditions that led to
that result. It should also record unusual system activities 
such as RAS events that might help in a post-mortem analysis 
when expected results are not obtained.
The new paradigm must produce such a record for every job.
Long running parameter studies or uncertainty quantification runs
require stability for long periods of time. 
% DONG: This can move to our model or design
%Private filesystem
%namespaces, as described above, enable applications to lock down their
%environment including dependent shared libraries so that the effects from
%system updates on the application are minimized.
%


%DONG: This can go to the design space discussion and desing/model
%\subsection{Zero Downtime}
%The impact of downtime becomes greater with a higher degree of integration
%of systems and services. Thus, \ngrm\ must be tolerant
%of hardware and software faults and failures.
%It will be designed to have no single point of failure and support
%version interoperability that allows {\em live} software upgrades.
%The technique will faciliate a rolling update across the center
%without impacting overall center availability and/or running workloads.
%
%\ngrm\ will support the notion of lightweight containers 
%and private filesystem namespaces for jobs, achieving 
%a higher degree of isolation between system software and user-visible 
%software. For example, the RM and the Operating System (OS)
%kernel might see one version of the root filesystem, while an application
%might see another. The decoupling and isolation will be the key to the 
%much desired ability to upgrade the software levels in the machine's
%root filesystem without affecting any of the
%libraries that an application might be using, and vice-versa. 
%Similarly, one can upgrade the system OS image at idle points between jobs,
%with user-level software remaining unchanged, or vice versa. The separation of
%concerns gives more flexibility to the organization in determining software
%update policy and in fact could allow users or code development teams to
%control the software levels affecting their application, independent of
%other applications.
%

\paragraph{Low Noise:}
As the number of processes in a parallel application increases, 
OS scheduling jitter affects their executions to a larger degree.
Minimizing
the user-space system software contribution to the OS jitter must be one 
of the primary goals of the new paradigm.  Thus, the new paradigm must supplant
the independent monitoring, remote shell, and cron
services that contribute to noise today. The integrated services will
allow users to dial up or down the verbosity and frequency of monitoring,
depending on their debug/monitoring needs versus their application's noise
sensitivity. Cron (periodic housekeeping) jobs and rsh (remote command
executions) can be performed through the RM to minimize their impact,
such as running them between jobs or synchronized across jobs. 
The new RM must also be flexible enough to allow implementation of other
strategies for reducing the impact of noise, such as scheduling all
system activity to a CPU core that is not shared with the application.


\paragraph{Fault Tolerance:}
As the new paradigm manages the entire computing facility,
the RM's tolerance to hardware and software faults and failures
is no longer optional.
Thus, the new RM must have no single point of failure. Further, it must support
version interoperability that allows {\em live} software upgrades
and facilitate a rolling update across the center
without negatively impacting overall availability of 
the facility and/or running workloads.


\paragraph{Security:}
The new paradigm must continue to support and strengthen privacy 
and integrity on
the network to limit vulnerability to attacks involving physical access
to a system or its networks.

%DONG: This can go to the design/concept
%Lightweight containers will tightly control the access that
%applications have to system resources. For example, it is possible to
%run the job in its own network namespace such that direct access to the
%system management network is unavailable, or limited by node firewall
%rules. \ngrm\ could also use the private network namespace to isolate
%jobs from each other on virtual private networks.  
%Containers with private filesystem namespaces can limit visibility of
%filesystems to jobs based on the site policy. 
%In combination with I/O
%forwarding software, \ngrm\ could squash all access to filesystems to
%the user id of the job so privilege escalation within the job such as
%escaping a container or obtaining root within the container would not
%change the user's identity when accessing the filesystem.
%


\paragraph{Research and Tool Friendly:}
The new paradigm needs to facilitate development and use of 
scalable run-time code development tools to improve the productivity
of users who must develop, debug, optimize, test and verify their code
on the next-generation systems. 
Similarly, it must
also facilitate research with the same goal in mind.
Specifically, the new paradigm must provide highly scalable infrastructure
and rich run-time interfaces
on which tools can build. Further, it must have 
an ability to capture and publish sanitized system data at all levels to facilitate the use of
this data in HPC research. 


\paragraph{Extensibility:}
By all means, such a paradigm shift is an ambitious goal. Thus, 
the new RM must be extensible and customizable to accelerate the shift
and provide features that lower the barrier of entry into the
community of developers supporting and extending the RM.
Plugins, when designed properly, can significantly help realize this vision,
without having to sacrifice the stability of the core RM software. 
With plugins, various RM subsystems can be replaced and extended.  
This can be the mechanism for individual
sites to customize their RM for their particular needs. 
In addition, the new RM must be designed for testability.
Ideally it should be possible to test new plugins or even a complete
new version of the RM system within the confines of a job created by
a production version.  This self-hosting capability would enable regular
RM regression and stress testing to be automated without dedicated test
resources or special production system arrangements.

%Components of \ngrm\ will be designed with the ability to capture and
%publish sanitized system data at all levels to facilitate the use of
%this data in current and future research.  The plugin and self-hosting
%features of the system will encourage experimentation and
%facilitate research activities by allowing new and experimental
%versions of the software to be run within a production instance.
%One of our design goals is also to facilitate the
%development and use of scalable runtime code development tools 
%on jobs on next-generation computing systems. This will not only serve 
%to lower the barrier to create and deploy useful tools, but will also reduce the
%profusion of one-off software systems in use on production systems.
%

\section{Design Space}
\label{sect:designspace}

Before going into the design and implementation details of \ngrm, we discuss 
some of the key design challenges that the new RM paradigm presents. 
They represent the main factors that \ngrm's new concepts and software design
must effectively address in realizing the vision and new capabilities described in 
Section~\ref{sect:vision}. 

\subsection{Design Challenges}
\label{sect:challenges}

\begin{itemize}
\item{\sl Multidimensional scale challenge:} The new paradigm demands that 
      the RM must manage the entire computing facility as one common pool of 
      resources. Compared to the traditional paradigm, this presents 
      fundamentally more difficult scale challenges to the RM design,
      not only in the concurrency of a single workload but along
      several other dimensions. As concurrency increases, every RM run-time
      service must scale and noise must be put at bay.  
      The number of jobs and resources 
      that the RM must manage will drastically increase; the amount 
      of run-time information that the RM must monitor, trace and store 
      will grow in the scaling limit of the facility.
      Thus, this challenge precludes any centralized design in an attempt to 
      gain a wider view over the resources at the facility. 

\item{\sl Diverse workload challenge:} The new paradigm must recognize that
      different applications have different performance-limiting factors,
      and this imposes more complex requirements to how the RM should
      model the compute resources. The traditional approach of modeling 
      resources as a collection of compute nodes will only work well when the
      application is compute-bound. Modern workloads have grown in their
      complexity, and even today, only a small fraction of modern applications
      is compute-bound.
       
\item{\sl Dynamic workload challenge:} Not only must the paradigm support 
      disparate performance limiters across different applications, but
      also must it suit varying performance limiters within 
      a single application. Our applications and their programming paradigm
      are becoming increasingly dynamic with different resource requirements
      at different phases.

\item{\sl Power challenge:} As one specific example of emerging resource types,
      power is becoming critical. When the computing
      facility becomes power-bound instead of compute-node-bound, the new
      paradigm must help it to schedule workloads based upon the 
      maximum power limit at any level at the facility. Thus, 
      the resource representation of the new RM must be generalized
      enough to model consumable resources like power.  
      
\item{\sl Scheduling challenge:} As more diverse attributes of resources
      are factored into scheduling, more stalls can occur in the schedule.
      For instance, $N$ compute nodes may sit idling simply because they do not meet
      the network proximity requirement for a job that requested 
      $N$ nodes all connected at a same lower-level switch. Thus, our design 
      must provide alternative ways to fill the stalls to meet this 
      challenge.

\item{\sl Backward compatibility challenge:} The new paradigm must also be
      able to model the traditional paradigm, as its small subset. This
      then provides our design with a straightforward path to 
      backward compatibility with legacy scripts from a traditional 
      paradigm such as \slurm.

\item{\sl Integration risk:} In the new paradigm, the RM must 
     integrate other software essential to the next-generation computation. 
     But with higher integration comes the risk of hard-wiring assumptions
     that later prove to be confining. That can force changes down the road
     that are inconsistent with the initial design. This motivates
     an extensible framework design. 
     
\item{\sl Higher downtime costs:} The impact of downtime under the 
     new paradigm becomes much greater: if not designed adequately, 
     a downtime can negatively affect the availability of
     a large portion of the facility and/or running workloads across it.
     Thus, the new paradigm must be tolerant of hardware and software faults 
     and failures with no single point of failure and must also support 
     live software upgrades.

\ifcomments
\marginpar{\tiny {\bf ned-review:}
How to reconcile conflict with security requirements?  For example, an attacker
might request a known-vulnerable kernel version.
{\bf jg:} Restrictions on this are explained in {\em Lighweight Virtualiation Model} paragraph.}
\fi
\item{\sl Separation-of-concerns challenge:} Many attributes of the new
     paradigm motivate a much higher degree of separation between
     the software level visible to applications and system-level 
     software. For example, the paradigm must be able to reconstruct the user-visible
     software level to provide better reproducibility of simulations while not
     locking the system software level. 
   
\item{\sl Security challenge:} As the new paradigm increasingly motivates
     a highly distributed, hierarchical software design, 
     the importance of security across and within the components becomes greater.

\item{\sl Productivity challenges:} The new paradigm must improve end-user 
     productivity in part through tightly-integrated support for development
     and use of scalable code development run-time tools and research.

\end{itemize}


\subsection{New Conceptual Models}
\label{sect:models}
In this section, we describe some of the primary conceptual models that will embody 
this new paradigm while addressing the multitude of design challenges. 
The models form the basis for the software design of \ngrm.

\ifcomments
\marginpar{\tiny {\bf ned-review:}
The discussion here doesn't really make much sense until you read section 4. I
would provide a forward reference to assure your readers that the concept will
be explained in more detail later on.}
\fi
\paragraph{Unified Job Model:}
Traditionally, a job is simply defined to be a resource allocation, 
a concept too weak to support the new paradigm. 
Rather, we unify the traditional job notion 
with the notion of a resource manager instance---an independent set of resource manager services.
The RM instance must be delegated the main responsibility of managing the resources allocated to the job.
Then, the unified job model becomes the foundation
on which to build a hierarchical, resource-management 
scheme to address the {\sl multidimensional scale challenge}.
In addition, an RM instance can implement compatibility mode with a particular 
traditional paradigm only over its own allocation, providing a straightforward path to address
the {\sl backward compatibility challenge}. 


\paragraph{Job Hierarchy Model:}
To scale the new paradigm in the scaling limit of the entire computing facility,
we must avoid a centralized approach: the new paradigm requires a hierarchical management scheme 
with a well-balanced, multi-level delegation structure. 
For this purpose, we use a tree-based job hierarchy model that has many proven advantages
for extreme scalability. 
In this model, a job is only required to manage its children jobs,
which would be only a small fraction of the total number of jobs that are run
across the entire computing facility. Further, several guiding principles
throughout the job hierarchy strike a balance between the management
responsibility of a parent job and delegation/empowerment of a child job:
\begin{itemize}
\item{\sl Parent bounding rule:} the parent job grants and confines
     the resource allocation of all of its children.
\item{\sl Child empowerment rule:} within the bound set by the parent,
     the child job is delegated the ownership of the allocation
     and becomes solely responsible for most efficient uses of the resources.
\item{\sl Parental consent rule:} the child job must ask its parent job
     when it wants to grow or shrink the resource allocation, 
     and it is up to the parent to grant the request.                   
\end{itemize}
In general, these rules enforce the first principle of the new paradigm: 
imposing highly complex resource bounds to guarantee the highest operational efficiency
at any level across the computing facility, while enabling most efficient execution
and scheduling of the workloads within these bounds.
At the same time, this model is the most fundamental design concept,
which forms the basis to address many of the design challenges including 
the {\sl multidimensional scale}, {\sl dynamic workload}, {\sl power}, and
{\sl scheduling challenges}. 


\paragraph{Generalized Resource Model:}
In the traditional paradigm, compute resources are modeled primarily as a collection of compute nodes,
a simplistic perspective ill-suited for the new paradigm. 
Today's applications are diverse with disparate limiting performance 
factors beyond floating point computation. Further, computing centers
are increasingly concerned about managing new resource types such as power
and shared persistent storage. The generalized resource model is our concept
to represent various resource types and their relationships 
that can impact how well applications perform and the computing
facility operates. Our generalized resource model also includes a
unified resource specification and description language. Speaking the
same resource description language for request specification 
provides transparency and fine-grained expressibility.
Our generalized resource model addresses not only the {\sl diverse workload}
and {\sl power challenges}, but the {\sl scheduling challenge}. More specifically, 
the unified language approach allows users to express their resource requests
more flexibly, e.g., using ranges or boolean expressions instead of hard amounts to allow requests to be fulfilled from several equivalent resource types.
This makes the scheduling granularity of jobs finer and more malleable.

\paragraph{Resource Allocation Elasticity Model:}
As our applications and their programming models are becoming increasingly
dynamic, the new paradigm must support an elasticity model where an existing
resource allocation can grow and shrink, depending on the current needs
of applications and/or the computing facility. We support the 
elasticity model within our job hierarchy framework above: a child job sends
a grow or shrink request to its parent, which can go up the job hierarchy
until all requisite constraints are known for this request. Also, combining
this with the generalized resource model, the elasticity can be expressed for
any resource including power consumption. Our elasticity model addresses
not only the {\sl dynamic workload} and {\sl power challenges}, but also
{\sl scheduling challenge}. When a significant schedule stall is created 
with no small jobs to backfill, some of the currently running
jobs can grow into these stalled resources and possibly complete sooner.  


\paragraph{Common Scalable Communication Infrastructure Model:}
Our scalability strategy with respect to a large number of compute nodes 
is to provide a common scalable communication framework within each
job. When a job is created, a secure, scalable overlay network with common
communication service is established across its allocated nodes.
Except for the root-level job, the existing communication session of the
parent job assists the child job with rapid creation of its own session.
A communication session is only aware of its parent and child and passes
the limited set of control information through this communication channel.
Thus, this model enables highly scalable communication within a job, 
while limiting communications between jobs, addressing both the 
{\sl multidimensional scale} and {\sl security challenges.} 
Further, this backbone per-job communication network supports 
many well-known bootstrap interfaces for distributed programs including
many MPI implementations as well as run-time tools, and thus in part
addresses the {\sl productivity challenges}. 


\paragraph{Self-Hosting Model:}
We use a self-hosting model to instantiate a new RM instance: the parent
is capable of launching a standalone copy of itself as a child job, but possibly
with different plugins. 
This makes it easier for developers or a quality assurance team to test new RM versions,
helping addressing the {\sl higher downtime costs} challenge. 
Further, self-hosting with new and experimental plugins 
encourages experimentation and facilitates research activities 
within a production instance, addressing the {\sl productivity challenges}, too.


\paragraph{Lightweight Virtualization Model:}
The lightweight virtualization model is our response to 
the {\sl higher downtime costs}, {\sl separation-of-concerns} and
{\sl security challenges}. Full-fledged virtualization techniques like Xen and
Kernel-based Virtual Machine (KVM) have many advantages for these design challenges, but
that approach has proven to be ineffective for HPC due in large part
its high overhead~\cite{VirtHPC}.
Instead, our virtualization strategy exploits Linux kernel-enforced
resource management and isolation mechanisms to launch applications in
containers with virtually no impact on performance~\cite{ContainerVirt}.
Within a container, private file system namespaces allow the system and
applications to have divergent file system views, and to access file
systems with different constraints. 
For example, the RM and other system software might be launched from
one version of the root file system with no access restrictions,
while an application might be launched from another with {\em setuid}
capability disabled.
The decoupling and isolation will be
the key to the much desired ability to upgrade the software levels in the
machine's root file system without affecting any of the libraries that
an application might be using, and vice-versa. Similarly, one can upgrade
the system OS image at idle points between jobs, with user-level software
remaining unchanged, or vice versa. The separation of concerns gives more
flexibility to the organization in determining software update policy
and in fact could allow users or code development teams to control the
software levels affecting their application, independent of other
applications.


The lightweight container approach will be used to tightly control the access
that applications have to system resources. For example, it is possible to
run the job in its own network namespace such that direct access to the
system management network is unavailable, or limited by container-specific
firewall rules.  A job could be further isolated on its own virtual private
network.
Containers with private file system namespaces can limit visibility of
file systems and other system resources to jobs based on the site policy,
significantly addressing the {\sl security challenge}.

\section{Next Generation Resource Manager}
\label{sect:ngrm}



%DONG: we probaly better off putting this into a table.

%In this section we begin to describe in detail design attributes
%of our RM software system that address the challenges discussed in
%Section~\ref{sect:challenges} using the models put forward in
%Section~\ref{sect:models}. We begin by describing some common terms
%used in our design and its description, then move on to a discussion
%of our high level design.
%
%\subsection{Terminology}
%
%\paragraph{comms session}
%An association among a set of nodes that enables secure, scalable,
%elastic, and fault-resilient communication services.
%Communication outside the session must pass through the session
%{\em control node}.  New sessions are spawned hierarchically.
%
%\paragraph{confinement}
%The enforcement of resource allocations such that a {\bf job} cannot
%consume more than was allocated.  Confinement of a job to its assigned
%resources is the responsiblility of the parent job.
%
%\paragraph{instance}
%An independent set of RM services configured for a resource subset.
%Each instance runs within its own {\bf comms session}.
%
%\paragraph{job}
%A time-bounded allocation of resources.
%A job request is submitted to a running job.
%When the scheduler determines that it is time for a job to run,
%a new {\bf instance} is created for it.  Jobs are thus organized
%herarchically.
%As a special case the {\em root job} (job 0) owns all resources and
%runs perpetually without a time bound.
%Jobs are uniquely identified by a hierarchical {\em jobid},
%with the root job on the left, e.g. {\tt 0.4234.6}.
%
%\paragraph{lightweight job} (LWJ)
%A {\bf job} submitted and recorded in the current {\bf instance}
%which does not result in a new instance.
%LWJ's are uniquely identified by their parent's jobid followed
%by a colon, followed by the their {\em lwjobid}, e.g. {\tt 0.4234.6:8}.
%
%%\subsection{Hierachical Job Model}
%\subsection{High Level Design}
%

%In this section, we present the high-level design of \ngrm that realizes
%the key conceptual models described in Section~\ref{sect:vision}.
%

\ifcomments
\marginpar{\tiny {\bf ned-review:}
With this sentence it becomes clear that the self-hosting model is a core
concept in the design of NGRM, and not just a nifty tool for NGRM developers.
The earlier sections on self-hosting should emphasize this aspect.}
\fi
To realize our conceptual models, \ngrm\ uses a divide-and-conquer
algorithm based on {\em job recursion}. As our {\em unified job model}
dictates, we design \ngrm\ so that every job is actually a full implementation
of the \ngrm\ system, and therefore users can submit new jobs to an existing 
job to access the full power of \ngrm\ for managing
resources assigned to it. The existing job and the new ones
then form the parent-children relationship according to our {\em job hierarchy model.}
In principle, our design enables this recursion
to occur indefinitely so that the resources in the entire
HPC center are {\em divided} and {\em conquered} up to any level 
suited well for the specific needs of the center and/or workloads. 
To simplify and generalize this scheme, we must first represent
a wide range of compute resources in a tree-based hierarchy.

Figure~\ref{fig:ResHierarchy} illustrates our representation for
a modern HPC center. As shown in this figure,
the root of the tree (\textsc{Center}) represents 
the entire center-wide resources. 
At the next level, these resources are
refined to be some number of clusters (\textsc{Cluster}),
the maximum power budget (\textsc{Power}), and
software licenses (\textsc{Licenses}). \textsc{Power}
and \textsc{Licenses} illustrate that
this scheme can easily represent a wide range of 
resource types as well as their relationships---i.e.,
{\em our generalized resource model}.
Expanding any of the second-level nodes, one can further refine
its resource distribution. In this case, zooming on
\textsc{Cluster 2} refines its resources into some cluster-local
file system (\textsc{Storage}) and an interconnect domain (\textsc{Core Switch}).
Next, the \textsc{Core SW} resource has some number of \textsc{Rack}
resources associated
with it, and an arbitrary \textsc{Rack} has some number of compute nodes
and its maximum \textsc{Power} budget. And similiar resource refinements
and relationships hold true at the node level as well. 


\begin{figure}
\centering
\includegraphics[scale=0.90]{../fig/resource-hierarchy}
\caption{Hierarchical View of Resources in an HPC Center}
\label{fig:ResHierarchy}
\end{figure}

We now walk through mapping our job recursion algorithm to this hierarchical
resource representation.
When \ngrm\ initializes, only a single {\em root} or {\em
bootstrap} job exists. Analogous to the
UNIX {\tt init} process, the root job is the only job
that does not have a parent job---i.e. it solely serves as the root of
the tree-based job heirarchy. 
%, and contains all the resources that
%must be managed. 
Instead,  the root job gets {\em global} information about entire compute resources,
users, and configuration from a scalable, peristent database that 
serves as our configuration repository.
To show this concept, Figure~\ref{fig:JobHierarchy}(a) overlays the root job ({\tt job 0})
on the entire resource tree. 

%This can be confusing
%In a sense,
%the configuration repository serves as the parent of the root job.


% LWJ hasn't been defined. , and finally an example
%of two {\em lightweight jobs} dividing the resources of a
%single node.

\begin{figure}
\begin{minipage}{0.5\linewidth}
  \begin{center}
  \includegraphics[scale=0.45]{../fig/job-hierarchy-job0}
  {\em (a)}
  \end{center}
\end{minipage}
\begin{minipage}{0.5\linewidth}
  \begin{center}
  \includegraphics[scale=0.45]{../fig/job-hierarchy-job0-x}
  {\em (b)}
  \end{center}
\end{minipage}

\begin{minipage}{0.5\linewidth}
  \begin{center}
  \includegraphics[scale=0.45]{../fig/job-hierarchy-job0-x-y}
  {\em (c)}
  \end{center}
\end{minipage}
\begin{minipage}{0.5\linewidth}
  \begin{center}
  \includegraphics[scale=0.45]{../fig/job-hierarchy-lwj}
  {\em (d)}
  \end{center}
\end{minipage}
\caption[Job Hierarchy Example]{{\small An example job hierarcy shown on top
of a resource hierarchy.
{\em (a)} Job 0 spans all resources,
{\em (b)} job 0.x runs across all of {\sc Cluster 2},
{\em (c)} job 0.x.y runs across nodes N1--5, and finally
{\em (d)} two lightweight jobs are instantiated on N5.}}
\label{fig:JobHierarchy}
\end{figure}

Users of the root job then submit all new job requests to it, which are scheduled
by the scheduler of the root job. 
Each child of the root job is itself a full \ngrm\ instance according to 
our {\em unified job model}, thus users of a child job can further submit new job requests 
to the child job. The child may have different, customized scheduler and environment 
in accordance with our {\em self-hosting model}.
As shown in Figure~\ref{fig:JobHierarchy}(b) and (c),
we have a single job ({\tt job 0.x}) managing the entire resources in a single cluster, 
and its child job ({\tt job 0.x.y}) running on a subset of that cluster's resources.
The root job and its progeny all have the following features:
\begin{itemize}
\item{A job {\em owner} and users in its access list who are able to run
      within the current job and/or submit new jobs;}
\item{A resource manager component configured with the list of
      resources allocated to the job, their topology and other information;}
\item{A scheduler that accepts and schedules new jobs;}
\item{A job database that records new, running, and completed child jobs;}
\item{An ``isolated'' communications framework with {\em gateway} functionality
      for relaying messages to the parent---i.e., following our {\em common scalable communication infrastructure model};}
\item{A distributed and featureful run-time environment capable of launching
      or assisting the launch of parallel applications and distributed tools;}
\item{An ``interactive'' node on which batch scripts and/or user logins
       are contained;}
\item{A resolvable domain name based on a unique job id;}
\item{A local monitoring domain;}
\end{itemize}

Following our {\em self-hosting model}, we design many of these components
via plugins or with plugin capabilities. We expect that several features will
be user-selectable during job submission. For example, the root
job may have a complex, distributed scheduler, whereas users may
want to choose between FIFO, backfill, or other simpler schedulers
depending on the in-job workload.

By default, only the owner of a job may submit new jobs or access
the run-time services of a running job.  While the ownership of a job
should not be changed, it will be possible for users to add other
users to the access list within their job, thus ``inviting'' the
submission of new jobs by others. The requirement for this feature
is clear when considering the root job, from which all other jobs
are spawned. This job will be owned by a privileged user such as
root, yet the system administrators will obviously want to open
up this root job for access to all users who should be able to
run jobs in the center.

\marginpar{\tiny {\bf ned-review:}
If a parent dies is the child "reparented" so it's data can still be reaped
(i.e. becomes an orphan in the UNIX process model)}

When a job terminates, either due to a time limit or the work
submitted for the job has completed, the job releases most resources
back to the parent job. The control functions of the job remain and
wait for an asynchronous {\em reap} operation from the parent. When
the job is reaped, the parent job reads in all data from the child's
local job including its resource database. The parent then instantiates that data in
its own databases. In this way, global information about jobs
percolates back up through the job hierarchy, eventually back to
the root job.  The root job periodically updates the global,
persistent databases.

In our job recursion, the so-called {\em base case} is a
single job in which a single parallel application is invoked. In 
this case, a fully functional child job is actually not needed,
and we introduce the concept of a {\em lightweight job} (LWJ) for efficient
resource use. An LWJ is 
submitted and runs on the resources of the local job, but does not result in
a new invocation of all the features of a full job. Where
\ngrm\ jobs are like processes in a UNIX process tree, LWJs 
are like threads running within a process.
The final case in Figure~\ref{fig:JobHierarchy} is an example
of two LWJs that divide the resources of a single node.


LWJs have access to a feature rich, distributed,
run-time environment with a shared key-value store, advanced
placement services, and a plugin interface that allows extension
of these services for unique requirements. This environment will
enable the quick deployment of advanced run-time features such as
fast parallel launch of MPI applications and seamless tool integration. 
Since LWJs use the same interface to job management as full
jobs, their existence, assigned resources, duration, etc.
will be recorded in the local job database for posterity.
To run an LWJ, a user must be the owner of the current
job or on the run-time access list for the local job.
% or a parent? I can't remember what we decided here...

\subsection{Comparison with Traditional Job Schedulers / Resource Managers}
\label{sect:comparison}

In this section, we draw comparisons between \ngrm's constructs and traditional terms
to show that the traditional paradigm can easily be reduced to our new paradigm. 
\paragraph{job} In traditional terms, a user submits a request to a batch scheduler running on a particular cluster
for an allocation of computing resources.  This request is added to a
queue of other such requests.  When the scheduler grants a request, a
job is begun and a set of resources are allocated to the job.  At that
point, one or more processes are launched to do the work of the job.
The traditional job will be recognized as the base case of the \ngrm\ job,
although without the unifed RM instance and hierarchy. 
In \ngrm, the request is submitted not to a batch scheduler managing one
cluster, but to another job (perhaps the root job) which manages a subset
of resources.
A scheduler running in that job allocates resources, and launches work as
another job.

\paragraph{job step}
Depending on the system, processes are launched by a remote launcher such
as {\em mpirun} or {\em srun}.  In \slurm\ terms, each set of tasks so launched are
known as a job step.  There can be one or more job steps active within
a job throughout the life of a job.  These can run sequentially or in
parallel, and can either run on dedicated or shared resources.
In \slurm, they are processed within the job by a dedicated FIFO-based
job step scheduler.
By first approximation, the job step maps to the \ngrm\ LWJ. 
Job scripts that invoke a series of job steps will translate
to a series of LWJs in an \ngrm\ job, processed by the regular \ngrm\ scheduler
and the same in-job resource management machinery that would be used to
launch regular jobs.  Because each job is endowed with the capabilities
of the full system, there is immense flexibility in how a job structures
its work internally.  It can select a scheduler plugin appropriate to the
task (or provide its own), express complex resource requirements and job
interdependencies, and run work as LWJ's or full isolated jobs.

\paragraph{reservations}
Historically, batch schedulers provide a means to reserve a set of
resources for exclusive use by a user or group of users.  These
reservations are typically created in advance and are offered as
Dedidcated Application Times (DATs) for users' exclusive use.
Often this process involves some manual setup and special announcements.
In \ngrm, job reservations will take the form of regular job (with a
future start time) under which user jobs will run.  As with all \ngrm\ jobs,
the owner of the DAT job (the DAT coordinator) is empowered to manage
user access to the DAT, instigate custom monitoring, custom scheduling, 
arrange for access to dedicated file systems, etc.
Thus in the new paradigm, the DAT is just another job that should not require
special handling.

\paragraph{job accounting}
In \slurm, the history of a job includes a detailed accounting of each job step
including the resources used and the duration of the job step.
Job and job step accounting statistics are commonly saved to a database.
In \ngrm, the LWJ accounting information and other data is saved to the
in-job job database.  Upon termination, the job's data is reaped by its
parent as described above, ultimately landing in the persistent copy
at the root job level.

\subsection{Project Organization and Thrust Areas}
\label{sect:projorg}
To hasten the design, development, and delivery of the initial version
of \ngrm\, we define four relatively independent thrust areas.
Each thrust area will carry out the vision and high-level design articulated
above with a focus on a particular subsystem or group of subsystems that
have a natural coupling.  The overall design of \ngrm\ will evolve and
require the whole team to weigh in on important changes, and the design
of interfaces between subsystems will require collaboration between
thrusts, but the work in each thrust can in large part progress
independently, building on the strength of the other thrusts.

Deliverables in each of the thrusts will be structured 
to leverage the framework approach to get the system up and running
early with simple plugins that are enhanced later.
Early prototypes will be used to accelerate the design of subsystem
interfaces and to obtain feedback from stakeholders and domain experts
as soon as possible.
Off-the-shelf components, external collaborations, and novel ideas from
the research and development communities will be leveraged where possible.
Thrust areas will adhere to a common set of project development practices
and standards (see Appendix~\ref{sect:process}).

Each of the thrust areas is briefly introduced below, then described in
more detail in the sections that follow.

\paragraph{Communication Framework}
The Communications Framework thrust will
realize our {\em common scalable communication infrastructure model}.
Building upon mature and portable Internet protocols and services,
the comms framework enables rich, scalable communications services
within a job and more limited communication between jobs using
the job's {\em control node} as gateway.
A {\em comms toolkit} provides a security model, tools for
encoding and decoding messages, and tools for transporting messages between
nodes using various messaging patterns including PUB-SUB (with multicast),
RPC, and streaming.
A {\em comms message broker} tracks node liveness, provides support for
building fault-tolerant services within the job,
and provides a multicast {\em scheduling trigger} used to synchronize
system overhead within the job to reduce noise.
A persistent {\em reduction network}
provides the structure needed to build tools and services that have
in-situ reduction capability, rooted at the control node.

\paragraph{Resource Management}

The Resource Management thrust encompasses the configuration,
scheduling, and tracking of resources, jobs, and users in the \ngrm\
system. To meet our new paradigm, this thrust must focus on making
the RM subsystem generalized, flexible, and extensible. To that
end, the \ngrm\ RM system will develop a domain-specific {\em
resource description language} which will be used to describe
the configuration, current state, and topological organization of
resources, while also being capable of describing {\em requests} for
those resources.  The RM thrust will also develop a set of scalable,
generalized databases or repositories to store information about
resources, jobs, and users -- one set as a global, persistent
datastore, and another short-lived set of databases at each job
level. At the global, persistent level a {\em resource repository}
will act as s top-level configuration database for resources. A
read-write copy of this repository will be available within each
job instance (at this level termed the {\em resource database}),
such that any job may modify resource attributes as if it were a
full instance of \ngrm. The global {\em job repository} will be
responsible for storing historical job information, including,
but not limited to, resources assigned to the job and all its
children jobs, job provenance records such as software levels
and job environment, and any RAS events or other monitoring data
associated with the job run. Within each job, a {\em job database}
temporarily serves as the job repository until job destruction,
and is also the interface to job submission, termination, and
alteration activities.  Finally, the global {\em user repository}
is used as a source for user-specific information such as UID, and
also possibly user preferences, roles, and so on. Within each job,
the local {\em user database} can be used to control permissions
for submission of new job requests, launching of lightweight jobs,
and other access control activities.

The scheduler is also a critical component of the RM thrust area,
as it is responsible for computing a job execution schedule based
on available resources, constraints posed by users in their job
request, and policy enforced by resource owners. The concept of
the resource description language will be extended to jobs with the
introduction of a {\em job description language} which is flexible
enough to describe complex job resource and time requirements. The
scheduler interface to \ngrm\ will be via a powerful plugin
subsystem, which will allow alternate job schedulers to be swapped
in, possibly on-demand, as jobs are launched in \ngrm. (For
instance, the root job may use an advanced fairshare scheduler,
while a job launched by a researcher for a parameter study may
incorporate a simpler scheduler tuned for high throughput). The
plugin interface for the scheduler should ease the development of
scheduler algorithms for third parties, and shall not require that
the scheduler software be built with access to \ngrm\ source code.
The scheduler interface will offer services that allow users
to query when their job might run, for example by exporting
the currently computed schedule as a diagram, or by estimating
a start time for a given job or set of jobs.

\paragraph{Monitoring}
Resource managers must monitor resource health to avoid scheduling
work on broken hardware.  \ngrm\ provides a comprehensive monitoring
environment including a {\em plugin framework} with data reduction
capability, synchronization of monitoring overhead across the job,
and the ability to tune the monitoring period and change the plugin
stack on a per-job basis.
A {\em fault notification system} enables system software, runtimes, and
applications to share fault information as a basis for building fault-tolerant
workloads and for recording interesting events within the job record.
\ngrm\ monitoring interfaces with an {\em external log database} intended
to support post-mortem analysis, and to
an {\em external enterprise monitoring system} such as might be used in
a site operations center.

\paragraph{Workload Runtime and Placement}
The \ngrm\ runtime launches the user workload such that it is confined
to allocated resources using a set of {\em confinement plugins}
and is provided a set of services intended to support application
runtimes and tools operating at extreme scale.
These services include a {\em distributed key value store} which facilitates
disseminating bootstrap information to distributed tools,
a {\em unified software bootstrap interface} which provides common
interfaces to a variety of tools and runtimes, and
a {\em job function synchronization} service which assists with the placement
of tool processes co-located with application processes.
The effort required to build a distributed tool on top of these services
and the comms layer will be greatly reduced from today, enabling
proof-of-concept research tools or even one-off tools to solve a particular
problem to be created in a short amount of time.

\include{comms}

\include{resmgmt}

\include{monitor}

\include{runtime}

%\include{project}

\appendix

\include{reqs}

%\include{process}

\bibliographystyle{abbrv}
\bibliography{../bib/project,../bib/rfc}

\end{document}
