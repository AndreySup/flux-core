\documentclass{article}

\usepackage{verbatim}
\usepackage{calc}
\usepackage{epsfig}
\usepackage{url}
\usepackage{draftcopy}
\usepackage{longtable}
\input{pstricks}
\usepackage{multirow}

\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{(\paperwidth-\textwidth)/2 - 1in}
\setlength{\topmargin}{(\paperheight-\textheight -\headheight-\headsep-\footskip)/2 - 1in + .5in }

\newcommand{\ngrm}{NGRM}
\newcommand{\ngrmfull}{Next Generation Resource Manager}
\newcommand{\ngjs}{NGRM Job Scheduler}
%\newcommand{\zMQ}{${\varnothing}$MQ}
\newcommand{\zMQ}{\O{}MQ}

%\includeonly{monitor}

\begin{document}

\title{Vision and Plan for a \ngrmfull}
\author{\
Dong Ahn, dahn@llnl.gov\\
Jim Garlick, garlick@llnl.gov\\
Mark Grondona, mgrondona@llnl.gov\\
Don Lipari, lipari@llnl.gov}

%\date{Nov 6, 2012}

\maketitle

\section{Overview}
\label{sect:overview}
Resource Management (RM) software is critical for High Performance Computing
(HPC). It is the centerpiece of efficient execution of HPC applications
while providing an HPC center with the main means to manage efficient use
of computing resources as a whole.  However, several growing trends make
even the best-in-breed RM software largely ineffective.  As numbers and
types of compute cores of an HPC machine continue to grow, key RM challenges
associated only with {\em capability} systems today are quickly
becoming pervasive for {\em all} computing resources, even commodity
Linux clusters.  These challenges include extreme scalability, power
consumption and noise control, fault tolerance, and heterogeneity management.

Greater difficulties in code development on larger scale systems have
also begun to impose far more complex requirements on the RM.  For example,
without adequate RM support, debugging, tuning, testing and verification
of applications are becoming increasingly time-consuming for end-users.
The next generation code development environments require the RM to provide
effective mechanisms to reproduce the results of program execution, to
provide accurate correlations between user-level errors and system-level
events, and to integrate and accelerate a rich set of scalable tools.

Further, a greater interplay among various classes of clusters across
the entire site makes the current practice of single-cluster scheduling
far less efficient.  An application running on a compute cluster heavily
utilizes site-wide shared resources such as I/O and visualization clusters.
Thus, avoiding any significant site-wide bottleneck now requires the RM
to schedule the job to all dependent resources together. In short, without
the RM that effectively meets all of these challenges, it has become apparent
that the HPC center will suffer a significant loss in both user productivity
and efficient use of next generation computing resources.

SLURM\cite{SlurmDesign} (Simple Linux Utilities for Resource Management) is arguably the
best-in-breed, open-sourced RM designed for commodity Linux clusters.
Livermore Computing (LC) at Lawrence Livermore National Laboratory (LLNL)
designed and led the implementation in 2002. And ever since, it has become
widely used beyond LC systems. Currently, most HPC centers including LC
run an instance of SLURM on each cluster and often tie these instances
together with grid management software such as MOAB\cite{MOAB:online}.  However, after a
decade of such use of SLURM and of accruing violations of many fundamental
technical assumptions, it has become no longer cost effective, if not
impossible, to meet newly emerging challenges without sacrificing its
stability and robustness.
% A couple of key things that support this SLURM argument? One or two
% points From the Gap analysis?] Time for a software rewrite has come.

Our response to this critical need is the \ngrmfull\ (\ngrm ), a RM software
{\em framework} that can address key emerging challenges 
in a simple, extensible, distributed and autonomous fashion.
It aims at managing the whole center as one common pool of {\em diverse} 
resources. Hence, scheduling decisions 
will be much more efficient and also flexible to be able to 
accommodate emerging constraints such as limited power budgets. 
Further, \ngrm\ integrates
system monitoring, system administration, lightweight
virtualization, and distributed tool communication capabilities
that are currently provided by disjoint and often overlapping software.
Integration of these facilities within a framework that is designed from
the ground up for scalability, security, and fault tolerance will result
in a more efficient and capable system.

We realize that \ngrm's problem domain is huge and that a wide range of challenges 
must be addressed in relatively short order with limited resources. Thus, we 
organize the project plan around four research and development thrust areas 
and systematically advance them.
Those four areas, {\bf{\em Resource Management}}, {\bf{\em Provenance and Monitoring}},
{\bf{\em Workload Runtime And Placement}}, and {\bf{\em Framework}}, are
relatively independent and that build on the strength of one
another through well-known interfaces and common design and development
principles. Reaching all major milestones of these thrust areas represent
the completion of the first round of \ngrm\ development. 

Overall, \ngrm\ will significantly improve operational efficiency for
scientific application development and execution, and further computing
resources of the HPC entire center.  It will also provide
a foundation that can be extended and customized, allowing agile responses
to site-specific scheduling issues. Perhaps more importantly, \ngrm\
positions HPC centers to cope with a blend of interrelated, diverse
extreme-scale computing resources, the landscape of high-end HPC centers
in just a few years down the road

The rest of the paper is organized as the following.
Section~\ref{sect:vision} describes \ngrm's vision and 
its new capabilities in more detail. Section~\ref{sect:projorg}  
presents the high-level goals of the four thrust areas and
explain relations among them. Next sections then 
go over and detail each of these thrust areas including 
its work breakdown structure (WBS) and work items.
Finally, Section~\ref{sect:delivery} defines
our schedules for milestones and deliverables.


\section{Challenges, Vision and New Capabilities}
\label{sect:vision}
With higher integration comes the risk of hardwiring assumptions that
later prove to be confining, forcing changes down the road that are
inconsistent with the initial design.  Therefore, the project will focus
on building a generalized framework that is highly customizable and
extensible.  It will be architected so that a great deal of its
functionality can be replaced without affecting the internal design.

\ngrm\ will improve productivity for bulk-synchronous, capacity workloads
through the lessons learned from a decade of designing, maintaining,
and running SLURM.  In addition, some new capabilities are planned which
are described below.

\subsubsection{Center as a Cluster}

Unlike the traditional approach of running a RM instance on each separately
managed cluster, and tying clusters together with grid software almost as
an afterthought, \ngrm\ will be designed from the beginning to manage the
center as one pool of resources.  There will be a single system image
from the perspective of users, as well as system administrators.

File system servers such as Lustre clusters, visualization systems, and
serial batch systems can be aggregated with compute clusters into one
management domain, such that the system can make better global scheduling
decisions.  With integrated, center-wide monitoring, it becomes easier
for the system to associate center RAS events such as a global file system
failure with a job that is affected and to make that information part of
the job’s data provenance record.  When the system has a global view of
resources including shared persistent storage, it becomes possible to
consider the scheduling of I/O along with computation.  In combination with
I/O forwarding software, the system could set up unique I/O forwarding
topologies for each job.

System management overhead that currently scales with the number of clusters
could be streamlined by aggregating per-cluster services into center wide
services.  For example, at LLNL, each cluster has a pair of management
nodes that are bootstrapped from scratch with an OS installation from DVD,
and when system software is updated on that cluster, a downtime is
scheduled during which the workload is idled and the responsible system
administrator performs a sequence of steps.  In the new system, the
number of systems that have to be brought up from scratch will be
greatly reduced, and administrator-orchestrated downtimes for software
updates could be eliminated (see below).

Dedicated interactive nodes will be unnecessary in the new system, as it
will be possible to allocate interactive environments on demand from the
global pool of resources.

\subsubsection{Zero Downtime}

The impact of downtime becomes greater with a higher degree of integration
of systems and services, therefore \ngrm\ will be designed to be tolerant
of failing hardware and software, with no single point of failure, and
with version interoperability such that software can be upgraded {\em live}
in a rolling update across the center without impacting overall center
availability or running workloads.

Using lightweight containers and private file system namespaces for jobs,
a degree of isolation between system software and user-visible software
can be obtained.  For example, the resource management software and the
kernel might see one version of the root file system, while an application
might see another.  This makes it possible for the software levels in
the system’s root file system to be updated without affecting any of the
libraries that might be demand-loaded by an application, and vice-versa.
The system OS image could be updated at idle points between jobs, while
user-level software remains unchanged, or vice versa.  The separation of
concerns gives more flexibility to the organization in determining software
update policy and in fact could allow users or code development teams to
control the software levels affecting their application, independent of
other applications.

\subsubsection{Low Noise}

Bulk-synchronous applications are affected to varying degrees by OS
scheduling jitter, depending on their communication patterns.  Minimizing
the user space system software contribution to OS jitter will be a primary
design goal of \ngrm.

\ngrm\ will supplant the independent monitoring, remote shell, and cron
services that contribute to noise today.  The integrated services will
allow users to dial up or down the verbosity and frequency of monitoring,
depending on the debug/monitoring needs versus their application’s noise
sensitivity.  Cron (periodic housekeeping) jobs and rsh (remote command
executions) can be performed through the system to minimize their impact,
such as running them between jobs or synchronized across jobs, and to
take into consideration the user's noise sensitivity.

\ngrm\ will also be flexible enough to allow implementation of other
strategies for reducing the impact of noise, such as scheduling all
non-application activity to a configurable {\em noise core} when a
single core can be spared.

\subsubsection{Data Provenance/Reproducibility}

As simulation plays an increasingly central role in scientific
investigation, reproducibility of results is more important than ever
before.  A result should be accompanied by a data provenance record that
can be used by others to recreate the inputs and conditions that led to
that result.  It should also record clues such as RAS events that might
help in a post-mortem analysis when expected results are not obtained.
The new system will produce such a record for every job.

Long running parameter studies or uncertainty quantification runs
require stability for long periods of time.  Private file system
namespaces, as described above, enable applications to lock down their
environment including dependent shared libraries so that the effects from
system updates on the application are minimized.

\subsubsection{Heterogeneous Resources}

Traditional RM's in HPC systems have focused on node and/or CPU centric
scheduling. With the advent of hybrid compute systems utilizing specialized,
heterogeneous resources, this model has become cumbersome. Rather than
perpetuating a node-centric resource model with support for generic
resources as an add-on, \ngrm\ will strive to support for heterogeneity
from the start.

In this new system, the idea of a resource will be kept as generic as
possible, not only allowing for simpler handling of generic resources,
but also enabling future expansion to resource types that have yet to
be conceived. New resource types will be coded in configuration and/or
extensions, inheriting their attributes from base types to maximize reuse
and foment collaboration.  Resource topology will be encoded via
configuration, and resources will also be allowed to have tags or labels
to which resource requests may refer. This system will attempt to
implement a generic resource request language to allow flexible
specification of resource requirements from users for their applications.

\subsubsection{Extensibility}

Plugins, when designed properly, minimize the knowledge of internals
needed to extend and modify a system.  They provide a mechanism for
sites to customize the system for their particular needs.  To the extent
possible, \ngrm\ will be modularized so various subsystems can be replaced
or extended with plugins.

In addition, \ngrm\ will be designed to be self-hosting; that is, it will
be capable of launching a standalone copy of itself as a job.  This makes
it possible for developers or a QA team to test new versions of the
system without requiring dedicated access to a cluster.

In combination, these features lower the barrier of entry into the
community of developers supporting and extending the system.

\subsubsection{Security}

Distributed components of \ngrm\ will support privacy and integrity on
the network to limit vulnerability to attacks involving physical access
to a system or its networks.

Lightweight containers enable the system to control the access that
applications have to system resources.  For example, it is possible to
run the job in its own network namespace such that direct access to the
system management network is unavailable, or limited by node firewall
rules.  \ngrm\ could also use the private network namespace to isolate
jobs from each other on virtual private networks.

Containers with private file system namespaces can limit visibility of
file systems to jobs based on site policy.  In combination with I/O
forwarding software, \ngrm\ could squash all access to filesystems to
the user id of the job so privilege escalation within the job such as
escaping a container or obtaining root within the container would not
change the user’s identity when accessing the file system.

\subsubsection{Research/Tool Friendly}

Components of \ngrm\ will be designed with the ability to capture and
publish sanitized system data at all levels to facilitate the use of
this data in current and future research.  The plugin and self-hosting
features of the system will serve to encourage experimentation and
facilitate research activities by allowing new and experimental
versions of the software to be run within a production instance.

During the design phase of \ngrm , detailed requirements for parallel
tools functionality will be gathered and incorporated into the final
software design. A major goal of this project is facilitate the
development and use of parallel tools on jobs running on next
generation HPC systems. This will not only serve to lower the
barrier to create and deploy useful tools, but will also reduce the
profusion of one-off software systems in use on production systems.

Overall, \ngrm\ will significantly improve operational efficiency for
scientific application development and execution, and further computing
resources of the HPC center as a whole.  In addition, it will provide
a foundation that can be extended and customized, allowing agile responses
to site-specific scheduling issues. Perhaps more importantly, \ngrm\ 
positions HPC centers to cope with a blend of interrelated, diverse
extreme-scale computing resources, the landscape of high-end HPC centers
in just a few years down the road.


\section{Project Organization and Thrust Areas}
\label{sect:projorg}
We realize that \ngrm 's problem domain is huge and that we must address
the challenges in relatively short order with limited resources, despite
the magnitude of the problem.  Therefore we define four thrust areas
that are relatively independent and build on the strength of one
another through well-known interfaces and common design and development
principles.  

Creating \ngrm\ will undoubtedly
require substantial collaborative efforts for research, design, development
and deployment, which involve many experts from different expert domains.
The software architecture must bolster rapid progress in all of our thrust
areas and facilitate mechanisms by which to leverage external innovation.

The four thrust areas are Resource Management, Provenance/Monitoring,
Workload Runtime, and Framework.

\subsubsection{Resource Management}

%This paragraph needs work to reflect revised thrust areas
\ngrm\ will investigate the notion of generalized resources so that
it can efficiently schedule a job to the resources that are beyond the
traditional definition of computing resources. An abstraction for
generalized resources will effectively handle a wide variety of computing
resources, from traditional compute cores and memory, to heterogeneous
computing elements, to consumable resources like power.
The highest-level of abstraction will allow the management of an
entire HPC center as one pool of resources.

\subsubsection{Provenance/Monitoring}

\ngrm\ will investigate methods of transparently generating provenance
information for each job, and enabling this information to be combined
with application-level provenance and user annotations to form
comprehensive documentation for simulation results.

\ngrm\ will research tools and mechanisms for encapsulating application
code and its software dependencies in immutable containers such that
this software can be held constant from run to run, aggressively
cached during job launch, and easily ported to other sites.

\ngrm\ will research scalable and extensible logging and monitoring
frameworks that have a tunable impact on system scheduling jitter and
can deliver high-volume, user-driven debug information to filters within
the job and/or stable storage for post-mortem analysis.

\ngrm\ will research scalable database technology to store
logs, fault information, and provenance data, which can be used
to correlate information from disparate sources for post-mortem debugging,
documenting provenance, and developing RAS metrics.

Finally, \ngrm\ will investigate fault notification mechanisms
to allow applications and system software to publish
faults which can trigger log dumps or a recovery response.

%\ngrm\ will create and advance provisioning systems that allow
%user control over file systems portion of the runtime environment
%for better reproducibility of simulation results.

\subsubsection{Workload Runtime}

\ngrm\ will develop and use a software design methodology to
guarantee that all software components our thrust areas produce
abide by a common set of desired attributes, including low noise,
high security, and fault containment with no single point of failure.

\subsubsection{Framework}
\ngrm\ will devise a secure and fault-tolerant backbone communication
infrastructure that can provide requisite scalability and flexible
capabilities for all \ngrm\ operations as well as the entire HPC
software ecosystem it embraces.

\ngrm\ will investigate and devise clean plugin interfaces that
provide \ngrm\ framework services and sanitized data to other HPC
stakeholders so that they can further extend and customize the benefits
of our system. \ngrm\ will identify, leverage, and support key complementary
projects and activities that seek solutions in the same problem domain
through innovations in other layers of HPC software stack, including but
not being limiting to middleware and programming models like MPI,
power-aware runtime, fault-tolerant components like checkpoint and
restart, OS services, development tools  and their infrastructure.
The plugins will be the main mean of extending \ngrm’s capability
without having to lose the stability and robustness of its core
functionality.

\include{framework}

\include{resmgmt}

\include{monitor}

\include{runtime}

\include{scheduler}

\include{project}

\appendix
\include{reqs}

\bibliographystyle{abbrv}
\bibliography{../bib/project,../bib/rfc}

\end{document}
