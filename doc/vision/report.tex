\documentclass{article}

\usepackage{verbatim}
\usepackage{calc}
\usepackage{epsfig}
\usepackage{url}
\usepackage{draftcopy}
\usepackage{longtable}
\input{pstricks}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{graphicx}

\graphicspath{ {../fig/} }

\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{(\paperwidth-\textwidth)/2 - 1in}
\setlength{\topmargin}{(\paperheight-\textheight -\headheight-\headsep-\footskip)/2 - 1.2in }

\newif\ifwbs
\wbsfalse

\newcommand{\ngrm}{NGRM}
\newcommand{\ngrmfull}{Next Generation Resource Manager}
\newcommand{\ngjs}{NGRM Job Scheduler}
%\newcommand{\zMQ}{${\varnothing}$MQ}
\newcommand{\zMQ}{\O{}MQ}
\newcommand{\slurm}{Slurm}
\newcommand{\moab}{Moab}

%\includeonly{monitor}

\begin{document}

\title{Vision and Plan for a \ngrmfull}
\author{\
Dong H. Ahn, ahn1@llnl.gov\\
Jim Garlick, garlick@llnl.gov\\
Mark Grondona, mgrondona@llnl.gov\\
Don Lipari, lipari@llnl.gov}

%\date{Nov 6, 2012}

\maketitle

\section{Overview}
\label{sect:overview}
Resource Management (RM) software is critical for High Performance Computing
(HPC). It is the centerpiece that allows efficient execution of HPC applications
while providing an HPC center with the main means to maximize 
the utilization of its computing resources. However, several growing trends make
even the best-in-breed RM software largely ineffective. As numbers and
types of compute cores of HPC systems continue to grow, key RM challenges
associated only with today's {\em capability-class} machines are 
becoming increasingly pervasive for {\em all} computing resources including 
commodity Linux clusters. The challenges include having to provide 
extreme scalability, low noise, fault tolerance, and heterogeneity management
while under a strict power budget.

In addition, greater difficulties in code development on larger systems have
begun to impose far more complex requirements on the RM. For example,
without adequate RM support, debugging, tuning, testing and verification
of the applications have become too difficult and time-consuming for end-users.
The next-generation code development environments require the RM to provide
effective mechanisms to support the reproducible results of program execution, to
provide accurate correlations between user-level errors and system-level
events, and to integrate and accelerate a rich set of scalable tools.

Further, a greater interplay among various classes of clusters across
the entire computing facility makes the current paradigm of single-cluster scheduling
largely ineffective. An application running on a compute cluster heavily
utilizes site-wide shared resources such as I/O and visualization clusters.
Thus, avoiding any significant site-wide bottleneck requires the RM
to schedule the job to all dependent resources together. In short, without
the RM that can effectively address all of these challenges, it has become apparent
that HPC centers will suffer a significant loss in both user productivity
and efficient uses of next-generation computing resources.

\slurm~\cite{SlurmDesign} is arguably the
best-in-breed, open-sourced RM designed for commodity Linux clusters.
Livermore Computing (LC) at Lawrence Livermore National Laboratory (LLNL)
designed and led its implementation in 2002 and since then has facilitated
its wide adoption outside of LLNL.
\slurm's original design was for a moderate-size Linux cluster with up to 2,000 
compute nodes connected in a single interconnect domain and MPICH-based, 
bulk-synchronous applications.
In the last decade, however, \slurm\ has been adapted to the various
environments that stray outside its core design.
It has adapted to act as a scheduling and job submission layer
on top of proprietary RM software on IBM Blue Gene and Cray systems,
to support other run-times quite different from MPICH,
to run on Linux clusters pushing far beyond the designed node count,
and to be tied into a \moab~\cite{MOAB:online} grid.
These adaptations have accrued without fundamentally changing the
core paradigm and design of \slurm. As a result, \slurm\ is increasingly
unable to adapt to emerging requirements without compromising
stability and code maintainability.

Our response to this critical need is the \ngrmfull\ (\ngrm ), an RM software
{\em framework} that can solve the key emerging challenges 
in a simple, extensible, distributed and autonomous fashion.
It aims at managing the whole center as one common pool of {\em diverse} 
resources. Hence, scheduling decisions will be 
far more efficient as well as flexible to accommodate 
emerging constraints such as a strict power bound. 
Further, \ngrm\ integrates
system monitoring, system administration, lightweight
virtualization, and distributed tool communication capabilities
that are currently provided by disjoint and often overlapping software.
Integration of these facilities within the common framework designed from
the ground up for scalability, security, and fault tolerance will result
in a more efficient and capable system.

We realize that \ngrm represents a paradigm shift for HPC resource management, 
and yet we must address a wide range of challenges in relatively short order
with limited resources. Thus, we 
organize our project plan around five research and development thrust areas 
and seek to advance them systematically.
These areas are called:
{\tt Resource Management},
{\tt Monitoring},
{\tt Workload Runtime And Placement}, 
{\tt Communications Framework}, and
{\tt I/O Management}.
They are relatively independent but can significantly build on the
strength of one another through well-known interfaces and common design
and development principles.
Thus, reaching all major milestones of these thrust areas 
will represent the completion of the first round of \ngrm\ development. 

Overall, \ngrm\ will significantly improve operational efficiency for
scientific application development and execution, and further for computing
resources of the entire HPC center.  It will also provide
a foundation for further extension and customization, allowing agile responses
to site-specific scheduling issues. Perhaps more importantly, \ngrm\
positions us to cope with a blend of interrelated, diverse
extreme-scale computing resources, the landscape of high-end HPC centers
in just a few years down the road.

The rest of the paper is organized as follows.
Section~\ref{sect:vision} presents \ngrm's vision and 
its new capabilities in more detail. 
In Section~\ref{sect:designspace}, we discuss 
the key software design challenges and the conceptual models
that embody the new paradigm while addressing these design challenges. 
Section~\ref{sect:ngrm} then describes our software design
and also highlights the five research and development thrust areas
and their relations. 
The following sections then go over and detail each of these areas including 
its work breakdown structure (WBS) and associated work items.
Finally, Section~\ref{sect:delivery} defines
our schedules for milestones and deliverables.


\section{Vision and New Capabilities}
\label{sect:vision}

The vision of \ngrm\ is to create a scalable RM software system that 
drastically improves operational efficiency and user productivity 
for workloads on {\em capacity-class} compute systems.
With a trend towards ever-growing numbers and types of compute cores, however,
this system class has been subject to the challenges that
today's {\em capability-class} machines have been facing. 
These challenges include having to provide extreme scalability, low noise, 
fault tolerance, and heterogeneity management while under a strict power budget.
Worse, the workloads themselves are also becoming increasingly diverse, 
dynamic, and large. Thus, fully realizing our vision through these challenges requires
a {\em paradigm shift} in how the RM should manage, model, schedule,
and allocate its resources.

In the new paradigm, the RM must be capable of imposing highly complex resource bounds
to guarantee the highest operational efficiency at any level
across the computing facility, while at the same time enabling most efficient execution
and scheduling of the workloads within these bounds.
Thus, the RM must manage the entire facility as one
common pool of resources. The ability to see a broader spectrum of resources 
and their various constraints can then lead to most efficient scheduling strategies
and execution environments. Further, the same ability will ease 
efforts to diagnose errors for both end users and support staff
by associating jobs with other facility-wide events. 
The new paradigm also demands that the RM model
various types of resources and their relationships beyond the traditional resource representation:
i.e., a simple collection of compute nodes.
The rich resource model will allow the RM to allocate computing resources
tailored to the disparate limiting factors of our applications: e.g.,
an application may be compute-bound while others are I/O-bound or power-bound.
Under the new paradigm, the resource allocations must also be elastic. 
An application may have different phases with disparate performance-limiting factors;
it must be able to grow and shrink its resource allocation dynamically. 
The global resource view, rich resource model, and elasticity represent
the fundamental characteristics of the new resource management paradigm.

Further, the new paradigm must provide a central framework to integrate
other relevant software. The software components should include   
system monitoring and administration, lightweight virtualization, 
and scalable tool communication. The integration will 
facilitate a higher level of leverage among these essential computing elements, 
and this will lead to significantly higher productivity 
for both end users and system administrators.  
In addition, as these capabilities are currently provided through disjoint
and often overlapping software, the integration will substantially reduce
the costs needed for developing and maintaining individual software.  
In the following, we further detail the key ideas and new capabilities 
under the new paradigm.


\paragraph{Center as a Cluster:}
Unlike the traditional paradigm of running an RM instance on each separately
managed cluster, 
%and tying clusters together with grid software almost was
%an afterthought. 
the new paradigm must manage the
entire computing facility as one pool of resources. There must be a single site-wide 
system image from the perspective of users as well as system administrators.
With this approach, file system servers such as Lustre clusters, 
and visualization and serial batch systems can be aggregated
with compute clusters into one management domain. Hence,
our RM can make better global scheduling decisions. 
Further, dedicated interactive nodes will be unnecessary in the new system, as it
will be possible to allocate interactive environments on demand from the
global pool of resources.

With integrated, site-wide monitoring, it becomes easier
for the new paradigm to associate global Reliability, Availability and Serviceability~(RAS) 
events such as a global file system
failure with an affected job and to make that information part of
the job's data {\em provenance} record. When the RM obtains a global view of
resources including shared persistent storage, it becomes possible to
consider the scheduling of I/O along with computation. In combination with
I/O forwarding software, our RM could easily set up unique I/O forwarding
topologies for each job.

This paradigm also has many advantages for system administration.
The system management overhead currently scales with the number of clusters.
But in this paradigm, per-cluster services can be aggregated into
site-wide services to reduce the overhead.
For example, each cluster at LLNL has a pair of management
nodes that are bootstrapped from scratch with an OS installation from DVD,
and when system software is updated on that cluster, a downtime is
scheduled during which the workload sits idle and the responsible system
administrator performs a sequence of steps. Under the new paradigm, the
number of systems that have to be brought up from scratch will be
greatly reduced, and administrator-orchestrated downtimes for software
updates will be eliminated. 


\paragraph{Diverse Compute Resources:}
The traditional paradigm has solely focused on node- and/or CPU-centric
scheduling. With the advent of hybrid compute systems utilizing specialized,
heterogeneous resources and also of other bounding resources like power, 
this simple resource model has become largely ineffective. 
Rather than
perpetuating a node-centric resource model with support for other
resources as an add-on, the new paradigm must embrace the concept of generalized resources: 
the idea of a resource is kept as generic as possible. This will not only facilitate 
simpler handling of diverse resources, but also enable future expansion to resource types that have yet to
be conceived. New resource types can be provided through configuration changes
and/or simple extensions that inherit their attributes from base types 
to maximize reuse and foment collaboration. Various resource topologies
can be encoded via configuration, and resources can also be given tags or labels
to which resource requests may refer. 
The new paradigm must also provide a generic resource query language to allow flexible
specification of resource requirements.


\paragraph{Data Provenance and Reproducibility:}
As simulation plays an increasingly central role in scientific
investigation, reproducibility of results is more important than ever
before. A result should be accompanied by a data provenance record that
can be used by others to reconstruct the inputs and conditions that led to
that result. It should also record unusual system activities 
such as RAS events that might help in a post-mortem analysis 
when expected results are not obtained.
The new paradigm must produce such a record for every job.
Long running parameter studies or uncertainty quantification runs
require stability for long periods of time. 
% DONG: This can move to our model or design
%Private filesystem
%namespaces, as described above, enable applications to lock down their
%environment including dependent shared libraries so that the effects from
%system updates on the application are minimized.
%


%DONG: This can go to the design space discussion and desing/model
%\subsection{Zero Downtime}
%The impact of downtime becomes greater with a higher degree of integration
%of systems and services. Thus, \ngrm\ must be tolerant
%of hardware and software faults and failures.
%It will be designed to have no single point of failure and support
%version interoperability that allows {\em live} software upgrades.
%The technique will faciliate a rolling update across the center
%without impacting overall center availability and/or running workloads.
%
%\ngrm\ will support the notion of lightweight containers 
%and private filesystem namespaces for jobs, achieving 
%a higher degree of isolation between system software and user-visible 
%software. For example, the RM and the Operating System (OS)
%kernel might see one version of the root filesystem, while an application
%might see another. The decoupling and isolation will be the key to the 
%much desired ability to upgrade the software levels in the machine's
%root filesystem without affecting any of the
%libraries that an application might be using, and vice-versa. 
%Similarly, one can upgrade the system OS image at idle points between jobs,
%with user-level software remaining unchanged, or vice versa. The separation of
%concerns gives more flexibility to the organization in determining software
%update policy and in fact could allow users or code development teams to
%control the software levels affecting their application, independent of
%other applications.
%

\paragraph{Low Noise:}
As the number of processes in a parallel application increases, 
OS scheduling jitter affects their executions to a larger degree.
Minimizing
the user-space system software contribution to the OS jitter must be one 
of the primary goals of the new paradigm.  Thus, the new paradigm must supplant
the independent monitoring, remote shell, and cron
services that contribute to noise today. The integrated services will
allow users to dial up or down the verbosity and frequency of monitoring,
depending on their debug/monitoring needs versus their application's noise
sensitivity. Cron (periodic housekeeping) jobs and rsh (remote command
executions) can be performed through the RM to minimize their impact,
such as running them between jobs or synchronized across jobs. 
The new RM must also be flexible enough to allow implementation of other
strategies for reducing the impact of noise, such as scheduling all
non-application activity to a configurable {\em noise core}.


\paragraph{Fault Tolerance:}
As the new paradigm manages the entire computing facility,
the RM's tolerance to hardware and software faults and failures
is no longer optional.
Thus, the new RM must have no single point of failure. Further, it must support
version interoperability that allows {\em live} software upgrades
and facilitate a rolling update across the center
without negatively impacting overall availability of 
the facility and/or running workloads.


\paragraph{Security:}
The new paradigm must continue to support and strengthen privacy 
and integrity on
the network to limit vulnerability to attacks involving physical access
to a system or its networks.

%DONG: This can go to the design/concept
%Lightweight containers will tightly control the access that
%applications have to system resources. For example, it is possible to
%run the job in its own network namespace such that direct access to the
%system management network is unavailable, or limited by node firewall
%rules. \ngrm\ could also use the private network namespace to isolate
%jobs from each other on virtual private networks.  
%Containers with private filesystem namespaces can limit visibility of
%filesystems to jobs based on the site policy. 
%In combination with I/O
%forwarding software, \ngrm\ could squash all access to filesystems to
%the user id of the job so privilege escalation within the job such as
%escaping a container or obtaining root within the container would not
%change the user's identity when accessing the filesystem.
%


\paragraph{Research and Tool Friendly:}
The new paradigm need to facilitate development and use of 
scalable run-time code development tools to improve end-user productivity
significantly, who must develop, debug, optimize, 
test and verify their code on the next-generation systems. 
Similarly, it must
also facilitate research with the same goal in mind.
Specifically, the new paradigm must provide highly scalable infrastructure
and rich run-time interfaces
on which tools can build. Further, it must have 
an ability to capture and publish sanitized system data at all levels to facilitate the use of
this data in HPC research. 


\paragraph{Extensibility:}
By all means, such a paradigm shift is an ambitious goal. Thus, 
the new RM must be extensible and customizable to accelerate the shift
and provide features that lower the barrier of entry into the
community of developers supporting and extending the RM.
Plugins, when designed properly, can significantly help realize this vision,
without having to sacrifice the stability of the core RM software. 
With plugins, various RM subsystems can be replaced and extended.  
This can be the mechanism for individual
sites to customize their RM for their particular needs. 
In addition, the new RM must be designed to be self-hosting: it must  
be capable of launching a standalone copy of itself as a job. This makes
it possible for developers or a quality assurance team to test new RM versions. 

%Components of \ngrm\ will be designed with the ability to capture and
%publish sanitized system data at all levels to facilitate the use of
%this data in current and future research.  The plugin and self-hosting
%features of the system will encourage experimentation and
%facilitate research activities by allowing new and experimental
%versions of the software to be run within a production instance.
%One of our design goals is also to facilitate the
%development and use of scalable runtime code development tools 
%on jobs on next-generation computing systems. This will not only serve 
%to lower the barrier to create and deploy useful tools, but will also reduce the
%profusion of one-off software systems in use on production systems.
%

\section{Design Space}
\label{sect:designspace}

Before going into the design and implementation details of \ngrm, we discuss 
some of the key design challenges that the new RM paradigm presents. 
They represent the main factors that \ngrm's new concepts and software design
must effectively address in realizing the vision and new capabilities described in 
Section~\ref{sect:vision}. 

\subsection{Design Challenges}
\label{sect:challenges}

\begin{itemize}
\item{\sl Multidimensional scale challenge:} The new paradigm demands that 
      the RM must manage the entire computing facility as one common pool of 
      resources. Compared to the traditional paradigm, this presents 
      fundamentally more difficult scale challenges to the RM design,
      not only in the concurrency of a single workload but along
      several other dimensions. As concurrency increases, every RM run-time
      service must scale and noise must be put at bay.  
      The number of jobs and resources 
      that the RM must manage will drastically increase; the amount 
      of run-time information that the RM must monitor, trace and store 
      will grow in the scaling limit of the facility.
      Thus, this challenge precludes any centralized design in an attempt to 
      gain a wider view over the resources at the facility. 

\item{\sl Diverse workload challenge:} The new paradigm must recognize that
      different applications have different performance-limiting factors,
      and this imposes more complex requirements to how the RM should
      model the compute resources. The traditional approach of modeling 
      resources as a collection of compute nodes will only work well when the
      application is compute-bound. Modern workloads have grown in their
      complexity, and even today, only a small fraction of modern applications
      is compute-bound.
       
\item{\sl Dynamic workload challenge:} Not only must the paradigm support 
      disparate performance limiters across different applications, but
      also must it suit varying performance limiters within 
      a single application. Our applications and their programming paradigm
      are becoming increasingly dynamic with different resource requirements
      at different phases.

\item{\sl Power challenge:} As one specific example of emerging resource types,
      power is becoming critical. When the computing
      facility becomes power-bound instead of compute-node-bound, the new
      paradigm must help it to schedule workloads based upon the 
      maximum power limit at any level at the facility. Thus, 
      the resource representation of the new RM must be generalized
      enough to model consumable resources like power.  
      
\item{\sl Scheduling challenge:} As more diverse attributes of resources
      are factored into scheduling, more stalls can occur in the schedule.
      For instance, $N$ compute nodes may sit idling simply because they do not meet
      the network proximity requirement for a job that requested 
      $N$ nodes all connected at a same lower-level switch. Thus, our design 
      must provide alternative ways to fill the stalls to meet this 
      challenge.

\item{\sl Backward compatibility challenge:} The new paradigm must also be
      able to model the traditional paradigm, as its small subset. This
      then provides our design with a straightforward path to 
      backward compatibility with legacy scripts from a traditional 
      paradigm such as \slurm.

\item{\sl Integration risk:} In the new paradigm, the RM must 
     integrate other software essential to the next-generation computation. 
     But with higher integration comes the risk of hard-wiring assumptions
     that later prove to be confining. That can force changes down the road
     that are inconsistent with the initial design. This motivates
     an extensible framework design. 
     
\item{\sl Higher downtime costs:} The impact of downtime under the 
     new paradigm becomes much greater: if not designed adequately, 
     a downtime can negatively affect the availability of
     a large portion of the facility and/or running workloads across it.
     Thus, the new paradigm must be tolerant of hardware and software faults 
     and failures with no single point of failure and must also support 
     live software upgrades.

\item{\sl Separation-of-concerns challenge:} Many attributes of the new
     paradigm motivate a much higher degree of separation between
     the software level visible to applications and system-level 
     software. For example, the paradigm must be able to reconstruct the user-visible
     software level to provide better reproducibility of simulations while not
     locking the system software level. 
   
\item{\sl Security challenge:} As the new paradigm increasingly motivates
     a highly distributed, hierarchical software design, 
     the importance of security across and within the components becomes greater.

\item{\sl Productivity challenges:} The new paradigm must improve end-user 
     productivity in part through tightly-integrated support for development
     and use of scalable code development run-time tools and research.

\end{itemize}


\subsection{New Conceptual Models}
\label{sect:models}
In this section, we describe some of the primary conceptual models that will embody 
this new paradigm while addressing the multitude of design challenges. 
The models form the basis for the software design of \ngrm.

\paragraph{Unified Job Model:}
Traditionally, a job is simply defined to be a resource allocation, 
a concept too weak to support the new paradigm. 
Rather, we unify the traditional job notion 
with the notion of a resource manager instance---an independent set of resource manager service.
The RM instance must be delegated the main responsibility of managing the resource allocated to the job.
Then, the unified job model becomes our logical basic-block
on which to build a hierarchical, resource-management 
scheme to address the {\sl multidimensional scale challenge}.
In addition, an RM instance can implement compatibility mode with a particular 
traditional paradigm only over its own allocation, providing a straightforward path to address
the {\sl backward compatibility challenge}. 


\paragraph{Job Hierarchy Model:}
To scale the new paradigm in the scaling limit of the entire computing facility,
we must avoid a centralized approach: the new paradigm requires a hierarchical management scheme 
with a well-balanced, multi-level delegation structure. 
For this purpose, we use a tree-based job hierarchy model that has many proven advantages
for extreme scalability. 
In this model, a job is only required to manage its children jobs,
which would be only a small fraction of the total number of jobs that are run
across the entire computing facility. Further, several guiding principles
throughout the job hierarchy strike a balance between the management
responsibility of a parent job and delegation/empowerment of a child job:
\begin{itemize}
\item{\sl Parent bounding rule:} the parent job grants and confines
     the resource allocation of all of its children.
\item{\sl Child empowerment rule:} within the bound set by the parent,
     the child job is delegated the ownership of the allocation
     and becomes solely responsible for most efficient uses of the resources.
\item{\sl Parentel consent rule:} the child job must ask its parent job
     when it wants to grow or shrink the resource allocation, 
     and it is up to the parent to grant the request.                   
\end{itemize}
In general, these rules enforce the first principle of the new paradigm: 
imposing highly complex resource bounds to guarantee the highest operational efficiency
at any level across the computing facility, while enabling most efficient execution
and scheduling of the workloads within these bounds.
At the same time, this model is the most fundamental design concept,
which forms the basis to address many of the design challenges including 
the {\sl multidimensional scale}, {\sl dynamic workload}, {\sl power}, and
{\sl scheduling challenges}. 


\paragraph{Generalized Resource Model:}
In the traditional paradigm, compute resources are modeled as a simple collection of compute nodes,
a simplified worldview ill-suited for the new paradigm. 
Today's applications are diverse with disparate limiting performance 
factors beyond the floating point computation. Further, the computing facility
is also increasingly subject to other resource types such as power
and shared persistent storage. The generalized resource model is our concept
to represent various resource types and their relationships 
that can impact how well applications perform and the computing
facility operates. Our generalized resource model also includes a
unified resource specification and description language. Speaking the
same resource description language for requirement specification 
provides transparency and fine-grained expressibility.
Our generalized resource model addresses not only the {\sl diverse workload}
and {\sl power challenges}, but the {\sl scheduling challenge}. More specifically, 
the unified language approach allows users to express their resource requirements
more flexibly:e.g., a range (instead of a hard amount) for certain resource type and/or OR operators
that allow choosing a resource from several equivalent resource types. This then makes the
schedule granule of jobs more malleable and finer. 


\paragraph{Resource Allocation Elasticity Model:}
As our applications and their programming models are becoming increasingly
dynamic, the new paradigm must support an elasticity model where an existing
resource allocation can grow and shrink, depending on the current needs
of applications and/or the computing facility. We support the 
elasticity model within our job hierarchy framework above: a child job sends
a grow or shrink request to its parent, which can go up the job hierarchy
until all requite constraints are known for this request. Also, combining
this with the generalized resource model, the elasticity can be expressed for
any resource including power consumption. Our elasticity model addresses
not only the {\sl dynamic workload} and {\sl power challenges}, but also
{\sl scheduling challenge}. When a significant schedule stall is created 
with no small jobs to backfill, some of the currently running
jobs can grow into these stalled resources and possibly complete sooner.  


\paragraph{Common Scalable Communication Infrastructure Model:}
Our scalability strategy with respect to a large number of compute nodes 
is to provide a common scalable communication framework within each
job. When a job is created, a secure, scalable overlay network with common
communication service is established across its allocated nodes.
Except for the root-level job, the existing communication session of the
parent job assists the child job with rapid creation of its own session.
A communication session is only aware of its parent and child and passes
the limited set of control information through this communication channel.
Thus, this model enables highly scalable communication within a job, 
while limits communications between other jobs, addressing both the 
{\sl multidimensional scale} and {\sl security challenges.} 
Further, this backbone per-job communication network supports 
many well-known bootstrap interfaces for distributed programs including
many MPI implementations as well as run-time tools, and thus in part
addresses the {\sl productivity challenges}. 


\paragraph{Self-Hosting Model:}
We use a self-hosting model to instantiate a new RM instance: the parent
is capable of launching a standalone copy of itself as a child job, but possibly
with different plugins. 
This makes it easier for developers or a quality assurance team to test new RM versions,
helping addressing the {\sl higher downtime costs}. 
Further, self-hosting with new and experimental plugins 
encourages experimentation and facilitates research activities 
within a production instance, addressing the {\sl productivity challenges}, too.


\paragraph{Lightweight Virtualization Model:}
The lightweight virtualization model is our response to 
the {\sl higher downtime costs}, {\sl separation-of-concerns} and
{\sl security challenges}. Full-fledged virtualization techniques like Xen and
Kernel-based Virtual Machine (KVM) have many advantages for these design challenges, but
that approach has proven to be ineffective for HPC due in large part
its high overhead. Instead, our virtualization strategy is through lightweight
mechanisms that modern operating systems like Linux offer.  Thus,
this model will exploit the notion of lightweight containers and private
file system namespaces for jobs, achieving a higher degree of isolation
between system software and user-visible software. For example, the RM
and the OS kernel might see one version of the root file system, while
an application might see another. The decoupling and isolation will be
the key to the much desired ability to upgrade the software levels in the
machine's root file system without affecting any of the libraries that
an application might be using, and vice-versa. Similarly, one can upgrade
the system OS image at idle points between jobs, with user-level software
remaining unchanged, or vice versa. The separation of concerns gives more
flexibility to the organization in determining software update policy
and in fact could allow users or code development teams to control the
software levels affecting their application, independent of other
applications.


The lightweight container approach will also tightly control the access
that applications have to system resources. For example, it is possible to
run the job in its own network namespace such that direct access to the
system management network is unavailable, or limited by node firewall
rules. This model will  also use the private network namespace to isolate
jobs from each other on virtual private networks.
Containers with private file system namespaces can limit visibility of
file systems to jobs based on the site policy, significantly addressing
the {\sl security challenge}.


\section{Next Generation Resource Manager}
\label{sect:ngrm}



%DONG: we probaly better off putting this into a table.

%In this section we begin to describe in detail design attributes
%of our RM software system that address the challenges discussed in
%Section~\ref{sect:challenges} using the models put forward in
%Section~\ref{sect:models}. We begin by describing some common terms
%used in our design and its description, then move on to a discussion
%of our high level design.
%
%\subsection{Terminology}
%
%\paragraph{comms session}
%An association among a set of nodes that enables secure, scalable,
%elastic, and fault-resilient communication services.
%Communication outside the session must pass through the session
%{\em control node}.  New sessions are spawned hierarchically.
%
%\paragraph{confinement}
%The enforcement of resource allocations such that a {\bf job} cannot
%consume more than was allocated.  Confinement of a job to its assigned
%resources is the responsiblility of the parent job.
%
%\paragraph{instance}
%An independent set of RM services configured for a resource subset.
%Each instance runs within its own {\bf comms session}.
%
%\paragraph{job}
%A time-bounded allocation of resources.
%A job request is submitted to a running job.
%When the scheduler determines that it is time for a job to run,
%a new {\bf instance} is created for it.  Jobs are thus organized
%herarchically.
%As a special case the {\em root job} (job 0) owns all resources and
%runs perpetually without a time bound.
%Jobs are uniquely identified by a hierarchical {\em jobid},
%with the root job on the left, e.g. {\tt 0.4234.6}.
%
%\paragraph{lightweight job} (LWJ)
%A {\bf job} submitted and recorded in the current {\bf instance}
%which does not result in a new instance.
%LWJ's are uniquely identified by their parent's jobid followed
%by a colon, followed by the their {\em lwjobid}, e.g. {\tt 0.4234.6:8}.
%
%%\subsection{Hierachical Job Model}
%\subsection{High Level Design}
%

%In this section, we present the high-level design of \ngrm that realizes
%the key conceptual models described in Section~\ref{sect:vision}.
%

To realize our conceptual models, \ngrm\ uses a divide-and-conquer
algorithm based on {\em job recursion}. As our {\em unified job model}
dictates, we design \ngrm\ so that every job is actually a full implementation
of the \ngrm\ system, and therefore new jobs can be submitted to a
local job to access the full power of \ngrm\ for managing
resources assigned to the job. In principle, our design enables this recursion
to occur indefinitely so that the resources in the entire
HPC center are {\em divided} and {\em conquered}
up to any level suited well for the specific needs of the center
or workloads. To simplify and generalize this scheme, we must first represent
various compute resources in a tree-based hierarchy with
as little exception as possible.


Figure~\ref{fig:ResHierarchy} illustrates our representation for
a modern HPC center. As shown in this figure,
the root of the tree (\textsc{Center}) represents 
the entire center-wide resources. 
At the next level, these resources can be
refined to be some number of clusters (\textsc{Cluster}),
the maximum power budget (\textsc{Power}),
software licenses (\textsc{Licenses}), etc. \textsc{Power}
and \textsc{Licenses} illustrate that
our scheme can easily represent a wide range of 
resource types as well as their relationships---i.e.,
{\em our generalized resource model}.
Cracking open any of the second-level nodes, one can further refine
its resource distribution. In this case, expanding on
\textsc{Cluster 2} refines its resources into some cluster-local
file system (\textsc{Storage}) and an interconnect (\textsc{Core Switch}).
Next, the \textsc{Core SW} resource has some number of \textsc{Rack}
resources associated
with it, and an arbitrary \textsc{Rack} has some number of compute nodes
and a \textsc{Power} budget, and so on.


\begin{figure}
\centering
\includegraphics[scale=0.75]{../fig/resource-hierarchy}
\caption{Hierarchical View of Resources in an HPC Center}
\label{fig:ResHierarchy}
\end{figure}

We now walk through mapping \ngrm's job recursion to this hierarchical
resource representation.
When \ngrm\ initializes, there exists a single {\em root} or {\em
bootstrap} job which contains all the resources that are managed by
\ngrm. This concept is shown overlaid on our hierarchical resource
tree in Figure~\ref{fig:JobHierarchy}a. The root job is the only job
that does not have a parent job, i.e. it serves as the root of
the heirarchical tree of jobs (an analogy in the UNIX idiom would
be that the root instance is the {\tt init} process.) Instead of
having a parent, the root job instead gets data about resources,
users, and configuration from a global, peristent database that 
serves as a configuration repository in \ngrm.

%This can be confusing
%In a sense,
%the configuration repository serves as the parent of the root job.

All new job requests are submitted to the root job and scheduled
by its scheduler. However, each child of the
root job is itself a full \ngrm\ instance
so job requests may now be directed at a child job.
As demonstrated in Figure~\ref{fig:JobHierarchy},
we have a single child job managing an entire cluster resource, a child job running
on a subset of that cluster's resources.

% LWJ hasn't been defined. , and finally an example
%of two {\em lightweight jobs} dividing the resources of a
%single node.

\begin{figure}
\begin{minipage}{0.5\linewidth}
  \begin{center}
  \includegraphics[scale=0.45]{../fig/job-hierarchy-job0}
  {\em (a)}
  \end{center}
\end{minipage}
\begin{minipage}{0.5\linewidth}
  \begin{center}
  \includegraphics[scale=0.45]{../fig/job-hierarchy-job0-x}
  {\em (b)}
  \end{center}
\end{minipage}

\begin{minipage}{0.5\linewidth}
  \begin{center}
  \includegraphics[scale=0.45]{../fig/job-hierarchy-job0-x-y}
  {\em (c)}
  \end{center}
\end{minipage}
\begin{minipage}{0.5\linewidth}
  \begin{center}
  \includegraphics[scale=0.45]{../fig/job-hierarchy-lwj}
  {\em (d)}
  \end{center}
\end{minipage}
\label{fig:JobHierarchy}
\caption[Job Hierarchy Example]{{\small An example job hierarcy shown on top
of a resource hierarchy.
{\em (a)} Job 0 spans all resources,
{\em (b)} job 0.x runs across all of {\sc Cluster 2},
{\em (c)} job 0.x.y runs across nodes N1--5, and finally
{\em (d)} two lightweight jobs are instantiated on N5.}}
\end{figure}

The root job and its progeny all have the following features:

\begin{itemize}
\item{A job {\em owner}, and access lists of users able to run
      within the current job and/or submit new jobs.}
\item{A resource manager component configured with the list of
      resources allocated to the job, their topology and other information.}
\item{A scheduler for accepting and scheduling new jobs.}
\item{A job database for recording new, running, and completed child jobs.}
\item{An ``isolated'' communications framework with {\em gateway} functionality
      for relaying messages to the parent.}
\item{A distributed and featureful runtime environment capable of launching
      or assisting the launch of parallel applications and tools.}
\item{An ``interactive'' node on which batch scripts and or user logins
       are contained.}
\item{A resolvable domain name based on a unique job id.}
\item{A local monitoring domain.}
\end{itemize}

To embody our {\em self-hosting model}, we design many of these components
via plugins or have plugin capabilities. We expect that several features will
be user-selectable during job submission. For example, the root
job may have a complex, distributed scheduler, whereas users may
want to choose between FIFO, backfill, or other simpler schedulers
depending on the in-job workload.

By default, only the owner of a job may submit new jobs or access
the run-time services of a running job.  While the ownership of a job
should not be changed, it will be possible for users to add other
users to the access list within their job, thus ``inviting'' the
submission of new jobs by others. The requirement for this feature
is clear when considering the root job, from which all other jobs
are spawned. This job will be owned by a privileged user such as
root, yet the system administrators will obviously want to open
up this root job for access to all users who should be able to
run jobs in the center.

When a job terminates, either due to a time limit or the work
submitted for the job has completed, the job releases most resources
back to the parent job. The control functions of the job remain and
wait for an asynchronous {\em reap} operation from the parent. When
the job is reaped the parent job reads in all data from the child's
local job and resource database and instantiates that data in
its own databases. In this way, global information about jobs
percolates back up through the job hierarchy, eventually back to
the root job.  The root job then periodically updates the global,
persistent databases.

In the recursive job model, the so-called {\em base case} is a
single job in which a single parallel application is invoked. For
this case, a fully functional child job is not needed, and indeed
would be a waste of resources. For this purpose we introduce the
concept of a {\em lightweight job} (LWJ) which is submitted and
runs on the resources of the local job, but does not result in
a new invocation of all the features of a full job. Where
\ngrm\ jobs are like processes in a UNIX process tree, lightweight jobs
are like threads running within a process.
The final case in Figure~\ref{fig:JobHierarchy} is an example
of two {\em lightweight jobs} dividing the resources of a
single node.


Lightweight jobs have access to a feature rich, distributed,
run-time environment with a shared key-value store, advanced
placement services, and a plugin interface that allows extension
of these services for unique requirements. This environment will
enable the quick deployment of advanced run-time features such as
fast parallel launch of MPI applications and easy tool integration.
Since LWJs use the same interface to job management as full
jobs, their existence, assigned resources, duration, etc.
will be recorded in the local job database for posterity.
To run an LWJ a user must be the owner of the current
job or on the runi-time access list for the local job.
% or a parent? I can't remember what we decided here...

\subsection{Comparison with Traditional Job Schedulers / Resource Managers}

While the above section serves to define the constructs of the \ngrm
system, Table~\ref{tab:tradterms} serves as a bridge to concepts used
in existing batch systems.

\begin{table}
\caption{Terminology Equivalents}
\centering
\begin{tabular}{|l|l|l|}
\hline
NGRM & SLURM & MPI \\
\hline
job & job & N/A\\
lightweight job (LWJ) & {\em srun} job step & {\em mpirun} MPI job\\
job & reservation & N/A \\
\hline
\end{tabular}
\label{tab:tradterms}
\end{table}

In traditional terms, a user submits a request to a batch scheduler
for an allocation of computing resources.  This request is added to a
queue of other such requests.  When the scheduler grants a request, a
job is begun and set of resources are allocated to the job.  At that
point, one to many tasks are launched to do the work of the job.

Depending on the system, tasks are launched by a remote launcher such
as {\em mpirun} or {\em srun}.  Each set of tasks so launched are
known as a job step.  There can be one to many job steps active within
a job throughout the life of a job.  These can run sequentially or in
parallel, and can either run on dedicated or shared resources.  The
history of the job includes a detailed accounting of each job step
including the resources used and the duration of the job step.

By first approximation, the traditional job maps to an NGRM job.  The
job step maps to a lightweight job (LWJ).  Job scripts that invoke a
series of job steps will translate to a series of LWJ's in an NGRM
job.

Some job scripts launch multiple job steps in parallel (backgrounded)
and a resource manager will schedule the associated task launches in a
FIFO basis, based on the available resources and whether dedicated or
shared resources were requested.  As described above, NGRM's job
construct will include the scheduler service.  This provides the user
even more flexibility to schedule the order of the LWJ instantiations
and task placement on resources.

Traditional job and job step accounting statistics are commonly saved
to a database.  In NGRM, the LWJ's statistics will be reaped by the
parent job as described above.

Historically, batch schedulers provide a means to reserve a set of
resources for exclusive use by a user or group of users.  These
reservations are typically created in advance and are offered as
Dedidcated Application Times (DATs) for users' exclusive use.  In
NGRM, job reservations will take the form of a parent job (with a
future start time) under which user jobs will run.

\subsection{Project Organization and Thrust Areas}
\label{sect:projorg}
To hasten the design, development, and delivery of the initial version
of \ngrm\, we define five relatively independent thrust areas.
Each thrust area will carry out the vision and high level design articulated
above with a focus on a particular subsystem or group of subsystems that
have a natural coupling.  The overall design of \ngrm\ will evolve and
require the whole team to weigh in on important changes, and the design
of interfaces between subsystems will require collaboration between
thrusts, but the work in each thrust can in large part progress
independently, building on the strength of the other thrusts.

Deliverables in each of the thrusts will be structured 
to leverage the framework approach to get the system up and running
early with simple plugins that are enhanced later.
Early prototypes will be used to accelerate the design of subsystem
interfaces and to obtain feedback from stakeholders and domain experts
as soon as possible.
Off-the-shelf components, external collaborations, and novel ideas from
the research and development communities will be leveraged where possible.
Thrust areas will adhere to a common set of project development practices
and standards (see Appendix~\ref{sect:process}).

Each of the thrust areas is briefly introduced below, then described in
more detail in the sections that follow.

\paragraph{Communication Framework}
The Communications Framework thrust will
realize our {\em common scalable communication infrastructure model}.
Building upon mature and portable Internet protocols and services,
the comms framework enables rich, scalable communications services
within a job and more limited communication between jobs using
the job's {\em control node} as gateway.
A {\em comms toolkit} provides a security model, tools for
encoding and decoding messages, and tools for transporting messages between
nodes using various messaging patterns including PUB-SUB (with multicast),
RPC, and streaming.
A {\em comms message broker} tracks node liveness, provides support for
building fault-tolerant services within the job,
and provides a multicast {\em scheduling trigger} used to synchronize
system overhead within the job to reduce noise.
A persistent {\em reduction network}
provides the structure needed to build tools and services that have
in-situ reduction capability, rooted at the control node.

\paragraph{Resource Management}

The Resource Management thrust encompasses the configuration,
scheduling, and tracking of resources, jobs, and users in the \ngrm\
system. To meet our new paradigm, this thrust must focus on making
the RM subsystem generalized, flexible, and extensible. To that
end, the \ngrm\ RM system will develop a domain-specific {\em
resource description language} which will be used to describe
the configuration, current state, and topological organization of
resources, while also being capable of describing {\em requests} for
those resources.  The RM thrust will also develop a set of scalable,
generalized databases or repositories to store information about
resources, jobs, and users -- one set as a global, persistent
datastore, and another short-lived set of databases at each job
level. At the global, persistent level a {\em resource repository}
will act as s top-level configuration database for resources. A
read-write copy of this repository will be available within each
job instance (at this level termed the {\em resource database}),
such that any job may modify resource attributes as if it were a
full instance of \ngrm. The global {\em job repository} will be
responsible for storing historical job information, including,
but not limited to, resources assigned to the job and all its
children jobs, job provenance records such as software levels
and job environment, and any RAS events or other monitoring data
associated with the job run. Within each job, a {\em job database}
temporarily serves as the job repository until job destruction,
and is also the interface to job submission, termination, and
alteration activities.  Finally, the global {\em user repository}
is used as a source for user-specific information such as UID, and
also possibly user preferences, roles, and so on. Within each job,
the local {\em user database} can be used to control permissions
for submission of new job requests, launching of lightweight jobs,
and other access control activities.

The scheduler is also a critical component of the RM thrust area,
as it is responsible for computing a job execution schedule based
on available resources, constraints posed by users in their job
request, and policy enforced by resource owners. The concept of
the resource description language will be extended to jobs with the
introduction of a {\em job description language} which is flexible
enough to describe complex job resource and time requirements. The
scheduler interface to \ngrm\ will be via a powerful plugin
subsystem, which will allow alternate job schedulers to be swapped
in, possibly on-demand, as jobs are launched in \ngrm. (For
instance, the root job may use an advanced fairshare scheduler,
while a job launched by a researcher for a parameter study may
incorporate a simpler scheduler tuned for high throughput). The
plugin interface for the scheduler should ease the development of
scheduler algorithms for third parties, and shall not require that
the scheduler software be built with access to \ngrm\ source code.
The scheduler interface will offer services that allow users
to query when their job might run, for example by exporting
the currently computed schedule as a diagram, or by estimating
a start time for a given job or set of jobs.

\paragraph{Monitoring}
Resource managers must monitor resource health to avoid scheduling
work on broken hardware.  \ngrm\ provides a comprehensive monitoring
environment including a {\em plugin framework} with data reduction
capability, synchronization of monitoring overhead across the job,
and the ability to tune the monitoring period and change the plugin
stack on a per-job basis.
A {\em fault notification system} enables system software, runtimes, and
applications to share fault information as a basis for building fault-tolerant
workloads and for recording interesting events with the job record.
\ngrm\ monitoring interfaces with an {\em external log database} intended
to support post-mortem analysis, and to
an {\em external enterprise monitoring system} such as might be used in
a site operations center.

\paragraph{Workload Runtime and Placement}
The \ngrm\ runtime launches the user workload such that it is confined
to allocated resources using a set of {\em confinement plugins}
and is provided a set of services intended to support application
runtimes and tools operating at extreme scale.
These services include a {\em distributed key value store} which facilitates
disseminating bootstrap information to distributed tools,
a {\em unified software bootstrap interface} which provides common
interfaces to a variety of tools and runtimes, and
a {\em job function synchronization} service which assists with the placement
of tool processes co-located with application processes.
The effort required to build a distributed tool on top of these services
and the comms layer will be greatly reduced from today, enabling
proof-of-concept research tools or even one-off tools to solve a particular
problem to be created in a short amount of time.

\paragraph{I/O Management}
\ngrm\ allows a job to customize its I/O configuration through the use
of {\em private namespaces}, and is capable of managing and configuring
I/O resources such as disks, file systems, file caches, and network channels
through plugins.
File {\em stage-in} and {\em stage-out}, e.g. for executables and
checkpoint files, is managed by a job-specific plugin.
An {\em io-analysis} plugin allows application file accesses and
I/O activity to be automatically analyzed in situ, monitored, and recorded.
Still another {\em io-qos} plugin permits custom I/O forwarding topologies
and file system quality-of-service to be established for the job's
particular I/O demands.  Finally, a  {\em local storage} plugin allows
jobs to manage raw storage hardware (disks, SSD's, and RAM) allocated
to them, creating and destroying file systems on the fly for exclusive
use of the job which can be the targets of stage-in/stage-out.

\include{framework}

\include{resmgmt}

\include{monitor}

\include{io}

\include{runtime}

\include{project}

\appendix

\include{reqs}

\include{process}

\bibliographystyle{abbrv}
\bibliography{../bib/project,../bib/rfc}

\end{document}
