\documentclass{article}

\usepackage{verbatim}
\usepackage{calc}
\usepackage{epsfig}
\usepackage{url}
\usepackage{draftcopy}
\usepackage{longtable}
\input{pstricks}
\usepackage{multirow}

\setlength{\textwidth}{6in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{(\paperwidth-\textwidth)/2 - 1in}
\setlength{\topmargin}{(\paperheight-\textheight -\headheight-\headsep-\footskip)/2 - 1in + .5in }

\newcommand{\ngrm}{NGRM}
\newcommand{\ngrmfull}{Next Generation Resource Manager}
\newcommand{\ngjs}{NGRM Job Scheduler}
%\newcommand{\zMQ}{${\varnothing}$MQ}
\newcommand{\zMQ}{\O{}MQ}
\newcommand{\slurm}{Slurm}
\newcommand{\moab}{Moab}

%\includeonly{monitor}

\begin{document}

\title{Vision and Plan for a \ngrmfull}
\author{\
Dong H. Ahn, ahn1@llnl.gov\\
Jim Garlick, garlick@llnl.gov\\
Mark Grondona, mgrondona@llnl.gov\\
Don Lipari, lipari@llnl.gov}

%\date{Nov 6, 2012}

\maketitle

\section{Overview}
\label{sect:overview}
Resource Management (RM) software is critical for High Performance Computing
(HPC). It is the centerpiece that allows efficient execution of HPC applications
while providing an HPC center with the main means to maximize 
the utilization of its computing resources. However, several growing trends make
even the best-in-breed RM software largely ineffective. As numbers and
types of compute cores of HPC systems continue to grow, key RM challenges
associated only with today's {\em capability-class} machines are 
becoming increasingly pervasive for {\em all} computing resources including 
commodity Linux clusters. The challenges include having to provide 
extreme scalability, low noise, fault tolerance, and heterogeneity management
while under a strict power budget.

In addition, greater difficulties in code development on larger systems have
begun to impose far more complex requirements on the RM. For example,
without adequate RM support, debugging, tuning, testing and verification
of the applications have become too difficult and time-consuming for end-users.
The next-generation code development environments require the RM to provide
effective mechanisms to support the reproducible results of program execution, to
provide accurate correlations between user-level errors and system-level
events, and to integrate and accelerate a rich set of scalable tools.

Further, a greater interplay among various classes of clusters across
the entire center makes the current practice of single-cluster scheduling
far less appealing. An application running on a compute cluster heavily
utilizes site-wide shared resources such as I/O and visualization clusters.
Thus, avoiding any significant site-wide bottleneck requires the RM
to schedule the job to all dependent resources together. In short, without
the RM that can effectively address all of these challenges, it has become apparent
that HPC centers will suffer a significant loss in both user productivity
and efficient uses of next-generation computing resources.

\slurm\cite{SlurmDesign} is arguably the
best-in-breed, open-sourced RM designed for commodity Linux clusters.
Livermore Computing (LC) at Lawrence Livermore National Laboratory (LLNL)
designed and led its implementation in 2002 and since then has facilitated
its wide adoption outside of LLNL.
\slurm's original design was for a 1-2K node Linux cluster with a single
interconnect domain and bulk-synchronous workload based on MPICH.
In the last decade, \slurm\ has been adapted
to act as a scheduling and job submission layer on top of proprietary
RM software on IBM Blue Gene and Cray systems,
to support other runtimes quite different from MPICH, and
to be tied into a \moab\cite{MOAB:online} grid.
These adaptations have accrued without fundamentally changing the
core \slurm\ design, and as a result we find ourselves increasingly
unable to adapt effectively to emerging requirements without compromising
stability.

Our response to this critical need is the \ngrmfull\ (\ngrm ), an RM software
{\em framework} that can solve the key emerging challenges 
in a simple, extensible, distributed and autonomous fashion.
It aims at managing the whole center as one common pool of {\em diverse} 
resources. Hence, scheduling decisions will be 
far more efficient as well as flexible to accommodate 
emerging constraints such as a strict power bound. 
Further, \ngrm\ integrates
system monitoring, system administration, lightweight
virtualization, and distributed tool communication capabilities
that are currently provided by disjoint and often overlapping software.
Integrations of these facilities within the common framework designed from
the ground up for scalability, security, and fault tolerance will result
in a more efficient and capable system.

We realize that \ngrm's problem space is huge and yet we must address a wide range 
of challenges in relatively short order with limited resources. Thus, we 
organize our project plan around four research and development thrust areas 
and seek to advance them systematically.
These areas are called:
{\bf{\em Resource Management}},
{\bf{\em Monitoring and Data Management}},
{\bf{\em Workload Runtime And Placement}}, and
{\bf{\em Communications Framework}}.
They are relatively independent but can significantly build on the
strength of one another through well-known interfaces and common design
and development principles.
Thus, reaching all major milestones of these thrust areas 
will represent the completion of the first round of \ngrm\ development. 

Overall, \ngrm\ will significantly improve operational efficiency for
scientific application development and execution, and further for computing
resources of the entire HPC center.  It will also provide
a foundation for further extension and customization, allowing agile responses
to site-specific scheduling issues. Perhaps more importantly, \ngrm\
positions us to cope with a blend of interrelated, diverse
extreme-scale computing resources, the landscape of high-end HPC centers
in just a few years down the road

The rest of the paper is organized as the following.
Section~\ref{sect:vision} presents \ngrm's vision and 
its new capabilities in more detail. Section~\ref{sect:projorg}  
describes the high-level goals of the four thrust areas and
explains relations among the areas. Next sections then 
go over and detail each of these areas including 
its work breakdown structure (WBS) and associated work items.
Finally, Section~\ref{sect:delivery} defines
our schedules for milestones and deliverables.


\section{Challenges, Vision, and New Capabilities}
\label{sect:vision}
The vision of \ngrm\ is to create a new RM software that 
significantly improves user productivity and operational efficiency 
for capacity-class workloads on compute resources that are increasingly 
diverse, dynamic, and large.
The lessons learned from a decade of designing, developing
and administrating \slurm must guide us in realizing this vision. 
Thus, \ngrm\ will manage the entire center as one common pool 
of diverse resources. Further, they will be annotated with much richer information 
including physical topology and communication costs among resources.
%---e.g., 
%on a compute node, core 0 is closer to 7 than 8, as 0 and 7 are on 
%a same socket; node A is farther to B than C as a packet sent from A has 
%to route through the additional top-level switch in a fat-tree network to reach C.
Using a richer and more diverse resource model,  
the scheduling decisions and execution environments will 
be made more efficient and flexible. 

\ngrm\ must also seamlessly integrate system monitoring
and administration, lightweight virtualization, and scalable tool 
communication, the capabilities that are currently provided through 
disjoint and often overlapping software. The integration must 
faciliate a higher level of leverage among these essential elements.
But with higher integration comes the risk of hardwiring assumptions 
that later prove to be confining. That can force changes down the road 
that are inconsistent with the initial design. Thus, we must extensively
investigate ways to build a generalized framework that is highly 
customizable and extensible. The framework architecture will allow
a great deal of \ngrm\ functionality to be replaced without 
affecting its internal design and stability.
In the following, we further detail the key ideas and new capabilities 
of \ngrm.

\subsection{Center as a Cluster}

Unlike the traditional approach of running an RM instance on each separately
managed cluster, and tying clusters together with grid software almost was
an afterthought. Instead, \ngrm\ will be designed from the beginning to manage the
center as one pool of resources. There will be a single site-wide 
system image from the perspective of users as well as system administrators.
With this approach, filesystem servers such as Lustre clusters, 
and visualization and serial batch systems can be aggregated
with compute clusters into one management domain. Hence,
our RM can make better global scheduling decisions. 
In addtion, dedicated interactive nodes will be unnecessary in the new system, as it
will be possible to allocate interactive environments on demand from the
global pool of resources.

With integrated, center-wide monitoring, it becomes easier
for our system to associate center Reliability, Availability and Serviceability~(RAS) 
events such as a global filesystem
failure with an affected job and to make that information part of
the job's data {\em provenance} record. When the RM obtains a global view of
resources including shared persistent storage, it becomes possible to
consider the scheduling of I/O along with computation. In combination with
I/O forwarding software, our RM could easily set up unique I/O forwarding
topologies for each job.

System management overhead that currently scales with the number of clusters
could be streamlined by aggregating per-cluster services into center-wide
services. For example, at LLNL, each cluster has a pair of management
nodes that are bootstrapped from scratch with an OS installation from DVD,
and when system software is updated on that cluster, a downtime is
scheduled during which the workload sits idle and the responsible system
administrator performs a sequence of steps. Under the new RM, the
number of systems that have to be brought up from scratch will be
greatly reduced, and administrator-orchestrated downtimes for software
updates could be eliminated. 


\subsection{Zero Downtime}

The impact of downtime becomes greater with a higher degree of integration
of systems and services. Thus, \ngrm\ must be tolerant
of hardware and software faults and failures.
It will be designed to have no single point of failure and support 
version interoperability that allows {\em live} software upgrades.
The technique will faciliate a rolling update across the center 
without impacting overall center availability and/or running workloads.

\ngrm\ will support the notion of lightweight containers 
and private filesystem namespaces for jobs, achieving 
a higher degree of isolation between system software and user-visible 
software. For example, the RM and the Operating System (OS)
kernel might see one version of the root filesystem, while an application
might see another. The decoupling and isolation will be the key to the 
much desired ability to upgrade the software levels in the machine's
root filesystem without affecting any of the
libraries that an application might be using, and vice-versa. 
Similarly, one can upgrade the system OS image at idle points between jobs,
with user-level software remaining unchanged, or vice versa. The separation of
concerns gives more flexibility to the organization in determining software
update policy and in fact could allow users or code development teams to
control the software levels affecting their application, independent of
other applications.

\subsection{Low Noise}

As the number of processes in a parallel application increases, 
their executions are affected to varying degrees by OS
scheduling jitter, depending on their communication patterns. Minimizing
the user-space system software contribution to OS jitter will be one 
of our primary design goals.  \ngrm\ will supplant the independent monitoring, 
remote shell, and cron
services that contribute to noise today. The integrated services will
allow users to dial up or down the verbosity and frequency of monitoring,
depending on their debug/monitoring needs versus their application's noise
sensitivity. Cron (periodic housekeeping) jobs and rsh (remote command
executions) can be performed through the RM to minimize their impact,
such as running them between jobs or synchronized across jobs. 
%, and to
%take into consideration the user's noise sensitivity.
\ngrm\ will also be flexible enough to allow implementation of other
strategies for reducing the impact of noise, such as scheduling all
non-application activity to a configurable {\em noise core}.

%when a
%single core can be spared.

\subsection{Data Provenance and Reproducibility}

As simulation plays an increasingly central role in scientific
investigation, reproducibility of results is more important than ever
before. A result should be accompanied by a data provenance record that
can be used by others to recreate the inputs and conditions that led to
that result. It should also record unusual system activities 
such as RAS events that might help in a post-mortem analysis 
when expected results are not obtained.
The new RM will produce such a record for every job.
Long running parameter studies or uncertainty quantification runs
require stability for long periods of time. Private filesystem
namespaces, as described above, enable applications to lock down their
environment including dependent shared libraries so that the effects from
system updates on the application are minimized.

\subsection{Heterogeneous Resources}

Traditional RM's in HPC systems have focused on node- and/or CPU-centric
scheduling. With the advent of hybrid compute systems utilizing specialized,
heterogeneous resources, this model has become less effective. Rather than
perpetuating a node-centric resource model with support for generic
resources as an add-on, \ngrm\ will strive to support for heterogeneity
from the start.
Thus, the idea of a resource will be kept as generic as
possible. That will not only faciliate simpler handling of generic resources,
but also enable future expansion to resource types that have yet to
be conceived. New resource types will provided through configuration changes
and/or simple extensions that inherit their attributes from base types 
to maximize reuse and foment collaboration.  Resource topology will be encoded via
configuration, and resources will also be allowed to have tags or labels
to which resource requests may refer. 
A generic resource query language will be developed to allow flexible
specification of resource requirements.

\subsection{Extensibility}

Plugins, when designed properly, minimize the knowledge of internals
needed to extend and modify a system. They provide a mechanism for
sites to customize the system for their particular needs. To the extent
possible, \ngrm\ will be modularized so various subsystems can be replaced
or extended with plugins.
In addition, \ngrm\ will be designed to be self-hosting: it will
be capable of launching a standalone copy of itself as a job. This makes
it possible for developers or a QA team to test new versions of the
RM without requiring dedicated access to a cluster.
In combination, these features lower the barrier of entry into the
community of developers supporting and extending the RM.

\subsection{Security}

Distributed components of \ngrm\ will support privacy and integrity on
the network to limit vulnerability to attacks involving physical access
to a system or its networks.
Lightweight containers will tightly control the access that
applications have to system resources. For example, it is possible to
run the job in its own network namespace such that direct access to the
system management network is unavailable, or limited by node firewall
rules. \ngrm\ could also use the private network namespace to isolate
jobs from each other on virtual private networks.  
Containers with private filesystem namespaces can limit visibility of
filesystems to jobs based on the site policy. 
%
%Dong: I don't undestand the following sentence. 
In combination with I/O
forwarding software, \ngrm\ could squash all access to filesystems to
the user id of the job so privilege escalation within the job such as
escaping a container or obtaining root within the container would not
change the user's identity when accessing the filesystem.

\subsection{Research and Tool Friendly}

Components of \ngrm\ will be designed with the ability to capture and
publish sanitized system data at all levels to facilitate the use of
this data in current and future research.  The plugin and self-hosting
features of the system will encourage experimentation and
facilitate research activities by allowing new and experimental
versions of the software to be run within a production instance.
%During the design phase of \ngrm , detailed requirements for parallel
%tools functionality will be gathered and incorporated into the final
%software design. 
One of our design goals is also to facilitate the
development and use of scalable runtime code development tools 
on jobs on next-generation computing systems. This will not only serve 
to lower the barrier to create and deploy useful tools, but will also reduce the
profusion of one-off software systems in use on production systems.

\section{Project Organization and Thrust Areas}
\label{sect:projorg}
To hasten the design, development, and delivery of the initial version
of \ngrm\, we define four thrust areas that are relatively independent
and build on the strength of one another through well-known interfaces
and common design and development principles (see Appendix~\cite{SWProcess}).

Creating \ngrm\ will require substantial collaborative efforts with
experts from several domains.
The software architecture must bolster rapid progress in all of our thrust
areas and facilitate mechanisms by which to leverage external innovation.

FIXME: describe functional architecture, leading into thrust areas

The four thrust areas are
Communication Framework,
Resource Management,
Monitoring and Data Management,
and Workload Runtime and Placement.

\subsection{Communication Framework}
\ngrm\ will devise a secure and fault-tolerant backbone communication
infrastructure that can provide requisite scalability and flexible
capabilities for all \ngrm\ operations as well as the entire HPC
software ecosystem it embraces.

\subsection{Resource Management}

%This paragraph needs work to reflect revised thrust areas
\ngrm\ will investigate the notion of generalized resources so that
it can efficiently schedule a job to the resources that are beyond the
traditional definition of computing resources. An abstraction for
generalized resources will effectively handle a wide variety of computing
resources, from traditional compute cores and memory, to heterogeneous
computing elements, to consumable resources like power.
The highest-level of abstraction will allow the management of an
entire HPC center as one pool of resources.

\subsection{Monitoring and Data Management}

\ngrm\ will investigate methods of transparently generating provenance
information for each job, and enabling this information to be combined
with application-level provenance and user annotations to form
comprehensive documentation for simulation results.

\ngrm\ will research tools and mechanisms for encapsulating application
code and its software dependencies in immutable containers such that
this software can be held constant from run to run, aggressively
cached during job launch, and easily ported to other sites.

\ngrm\ will research scalable and extensible logging and monitoring
frameworks that have a tunable impact on system scheduling jitter and
can deliver high-volume, user-driven debug information to filters within
the job and/or stable storage for post-mortem analysis.

\ngrm\ will research scalable database technology to store
logs, fault information, and provenance data, which can be used
to correlate information from disparate sources for post-mortem debugging,
documenting provenance, and developing RAS metrics.

Finally, \ngrm\ will investigate fault notification mechanisms
to allow applications and system software to publish
faults which can trigger log dumps or a recovery response.

%\ngrm\ will create and advance provisioning systems that allow
%user control over file systems portion of the runtime environment
%for better reproducibility of simulation results.

\subsection{Workload Runtime}

\ngrm\ will develop and use a software design methodology to
guarantee that all software components our thrust areas produce
abide by a common set of desired attributes, including low noise,
high security, and fault containment with no single point of failure.

\include{framework}

\include{resmgmt}

\include{monitor}

\include{runtime}

\include{project}

\appendix

\include{reqs}

\include{process}

\bibliographystyle{abbrv}
\bibliography{../bib/project,../bib/rfc}

\end{document}
